\chapter{Convexity analysis of the logistic regression}\label{ch:B}


A function $f(\cdot)$ which is twice-differentiable is convex if and only if its hessian 
matrix (matrix of second-order partial derivatives) is positive semi-definite \citep{Boyd2010}.
Therefore, with the objective of evaluating the convexity of the logistic function, the first 
order partial derivatives are calculated as follows:

\begin{equation}
\left[ \begin{array}{c}
  \frac{\partial J(\theta)}{\partial \theta_1} \\[0.1in]  
  \frac{\partial J(\theta)}{\partial \theta_2} \\[0.1in]  
  \ldots \\[0.1in]  
  \frac{\partial J(\theta)}{\partial \theta_k}
\end{array} \right] =
\left[ \begin{array}{c}
  \frac{1}{N}\sum_{i=1}^{N}\left[x_i^{(1)}\left(h_\theta(\mathbf{x}_i)-y_i\right)\right]\\[0.1in]
  \frac{1}{N}\sum_{i=1}^{N}\left[x_i^{(2)}\left(h_\theta(\mathbf{x}_i)-y_i\right)\right]\\[0.1in]
  \ldots \\[0.1in]  
  \frac{1}{N}\sum_{i=1}^{N}\left[x_i^{(k)}\left(h_\theta(\mathbf{x}_i)-y_i\right)\right]\\[0.1in] 
\end{array} \right]
\end{equation}
where $x_i^{(k)}$ is the k element of the feature vector $\mathbf{x}_i$ of example $i$.
Then, using the first order partial derivatives, the Hessian ($H$) can be generalized as
\begin{equation}
H=\left[ \begin{array}{cccc}
  \partialb{1}{1} & \partialb{1}{2} & \cdots & \partialb{1}{k} \\[0.1in]  
  \partialb{2}{1} & \partialb{2}{2} & \cdots & \partialb{2}{k} \\[0.1in]  
  \cdots & \cdots & \cdots & \cdots \\[0.1in]  
  \partialb{k}{1} & \partialb{k}{2} & \cdots & \partialb{k}{k}  
\end{array} \right]
\end{equation}
where each element of the matrix is calculated using

\begin{equation}
  \partialb{j1}{j2}=\frac{1}{N}\sum_{i=1}^{N}
  \left[\bigg(1-h_\theta(\mathbf{x}_i)\bigg)h_\theta(\mathbf{x}_i)x_i^{(j1)}x_i^{(j2)}\right],
\end{equation}
where $j1$ and $j2$ $\in \{1,\cdots,k\}$. 

For the cost function to be convex the Hessian matrix must be positive-semidefinite, and
a function is positive-semidefinite if:  
\begin{equation}
  z^T\left[\nabla_x^2f(x)\right]z \ge 0 \quad  \forall z,
\end{equation}
where $\nabla^2f(\cdot)$ is the second order derivate of $f(\cdot)$. Then, applied to J($\theta$), 
we need to prove that
\begin{equation}\label{eq:7:pos1}
  z^T\left[H \right]z \ge 0 \quad  \forall z.
\end{equation}

\noindent {\bf Proof: }
$H$ can be rewritten, using only the internal part of the sum, since it does not 
dependent on $\theta$
\begin{equation}
 H = \frac{1}{N}\sum_{i=1}^{N} \left[ 
\bigg(1-h_\theta(\mathbf{x}_i)\bigg)h_\theta(\mathbf{x}_i)\mathbf{x}_i^T \mathbf{x} \right]. 
\end{equation}
Then, 
\begin{align}
  z^THz = z^T \left[\bigg(1-h_\theta(\mathbf{x}_i)\bigg)h_\theta(\mathbf{x}_i)\mathbf{x}_i^T 
\mathbf{x}\right] z,
\end{align}
which can be rewritten as
\begin{align}
  z^THz = (1-h_\theta(\mathbf{x}_i))h_\theta(\mathbf{x}_i)\left(\mathbf{x}_i^Tz\right)^2.
\end{align}
Therefore,  $J(\theta)$ is convex, as $\left(\mathbf{x}_i^Tz\right)^2 
\ge 0$ and \\ $\left((1-h_\theta(\mathbf{x}_i))h_\theta(\mathbf{x}_i)\right)\ge 0$ because $0\ge 
h_\theta(\mathbf{x}_i) \ge 1$.
