\chapter{\textit{Costcla}: A Cost-sensitive classification library in \textbf{Python}}\label{ch:A}


\begin{center}
 \includegraphics[scale=0.25]{costcla_logo}
\end{center}

\begin{remark}
\textit{CostCla} is a \textbf{Python} open source cost-sensitive classification library built on 
top of \textit{Scikit-learn}, \textit{Pandas} and \textit{Numpy}. \textit{CostCla} provides a wide 
range of state-of-the-art example-dependent cost-sensitive methods for binary classification tasks. 
Source code, binaries and documentation are distributed under 3-Clause BSD license in the website 
\url{http://albahnsen.com/CostSensitiveClassification/}.
\end{remark}
  
\section{Library overview}

\textit{CostCla} is a easy to use \textbf{Python} library for example-dependent cost-sensitive 
classification problems. It includes many example-dependent cost-sensitive algorithms. Since 
it is part of the scientific \textbf{Python} ecosystem, it can be easily integrated with other 
machine learning libraries.

\textbf{Python} offers several machine learning packages, some including classification algorithms 
like \textit{Scikit-learn} \citep{Pedregosa2011}, \textit{PyBrain} \citep{Schaul2010}, 
\textit{PyMC} \citep{Patil2010}, \textit{mlpy} \citep{Albanese2012} and \textit{Orange} 
\citep{Demsar2013}. Nevertheless, all these packages are based on a cost-insensitive classification
 framework, in which all misclassification errors carry the same cost. As has been highlighted 
 throughout this thesis, this is not the case in many real-world applications. 

The core of the library consists of a number of state-of-the-art example-dependent cost-sensitive 
classification algorithms, such as cost-proportionate rejection-sampling \citep{Zadrozny2003}, 
cost-proportionate over-sampling \citep{Elkan2001}, Bayes minimum risk (see Chapter~\ref{ch:6}), 
cost-sensitive logistic regression (see Chapter~\ref{ch:7}), cost-sensitive decision trees (see 
Chapter~\ref{ch:8}), and the cost-sensitive ensemble methods: cost-sensitive bagging, cost-sensitive 
pasting, cost-sensitive random forest and cost-sensitive random patches (see Chapter~\ref{ch:9}).

Moreover, the library also includes three different example-dependent cost-sensitive databases. In 
particular two credit scoring databases (see Section~\ref{sec:4:creditscoring}), and one direct marketing
 database (see Section~\ref{sec:5:directmarketing}). 

\section{Usage}

In this section we provided a quick example of the usage of the \textit{CostCla}  library, and 
compare the results of different algorithms using a credit scoring database.

\vskip0.3cm
\noindent \textbullet\ Prepare dataset and load libraries

\vskip0.3cm
\begin{pythoncode}
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import train_test_split
from costcla.datasets import load_creditscoring2
from costcla.sampling import cost_sampling
from costcla.metrics import savings_score
from costcla import models
data = load_creditscoring2()
X_train, X_test, y_train, y_test, 
cost_mat_train, cost_mat_test = \
train_test_split(data.data, data.target, data.cost_mat)
\end{pythoncode}

\vskip0.3cm
\noindent\textbullet\ Random forest

\vskip0.3cm
\begin{pythoncode}
f_RF = RandomForestClassifier()
f_RF.fit(X_train, y_train)
y_pred = f_RF.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.042197359989}

\vskip0.3cm
\noindent \textbullet\ Cost-proportionate rejection sampling

\vskip0.3cm
\begin{pythoncode}
X_cps_r, y_cps_r, cost_mat_cps_r =  \ 
cost_sampling(X_train, y_train, cost_mat_train, 
              method='RejectionSampling')
f_RF.fit(X_train, y_train)
y_pred = f_RF.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.280743761779}

\newpage
\noindent \textbullet\ Bayes minimum risk

\vskip0.3cm
\begin{pythoncode}
f_RF.fit(X_train, y_train)
y_prob_test = f_RF.predict_proba(X_test)
f_BMR = models.BayesMinimumRiskClassifier()
f_BMR.fit(y_test, y_prob_test)
y_pred = f_BMR.predict(y_prob_test, cost_mat_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.285102564249}

\vskip0.3cm
\noindent \textbullet\ Cost-sensitive decision tree

\vskip0.3cm
\begin{pythoncode}
f_CSDT = models.CSDecisionTreeClassifier()
f_CSDT.fit(X_train, y_train, cost_mat_train)
y_pred = f_CSDT.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.289489571352}

\vskip0.3cm
\noindent \textbullet\ Cost-sensitive random patches

\vskip0.3cm
\begin{pythoncode}
f_CSRP = models.CSRandomPatchesClassifier()
f_CSRP.fit(X_train, y_train, cost_mat_train)
y_pred = f_CSRP.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.306607400467}

\vskip0.3cm
In \tablename{~\ref{tab:9:1}} we summarize the results of the different 
algorithms. It is observed that the library follows the same API structure of 
\textit{Scikit-learn}, therefore, making \textit{CostCla} highly compatible with the most widely 
used machine learning library in \textbf{Python}.

\begin{table}[htbp]
    \centering
    \footnotesize
    \begin{tabular}{l|c}
      Algorithm & Savings \\
      \hline
      Random forest & 0.0422 \\
      Cost-proportionate rejection sampling & 0.2807\\
      Bayes minimum risk & 0.2851\\
      Cost-sensitive decision tree & 0.2895\\
      Cost-sensitive random patches & 0.3066\\   
    \end{tabular}
    \caption{Results of the different algorithms using the \textit{CostCla} library}
    \label{tab:9:1}
  \end{table}  

\newpage
\section{Installation}

\textit{CostCla} requires some prerequisite packages to be previously installed. Nevertheless, all 
are cross-platform an freely available online:
\begin{itemize}
 \item \textbf{Python} version $\ge$ 2.7
 \item \textit{Numpy} version $\ge$ 1.8.0
 \item \textit{Pandas} version $\ge$ 0.14.0
 \item \textit{Scikit-learn} version $\ge$ 0.15.0b2
 \item \textit{pyea} version $\ge$ 0.1
\end{itemize}

\noindent The easiest way to install \textit{CostCla}  is with \textit{pip}:
\vskip0.3cm
\begin{pythoncode}
pip install costcla
\end{pythoncode}


