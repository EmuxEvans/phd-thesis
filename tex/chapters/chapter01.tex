\chapter{Introduction}\label{ch:1}

\section{Motivation and context}

  Classification, in the context of machine learning, deals with the problem of 
  predicting the class of a set of examples given their features. Traditionally, classification 
  methods aim at minimizing the misclassification of examples in which an example is 
  misclassified if the predicted class is different from the true class. Such a traditional 
  framework assumes that all misclassification errors carry the same cost. This is not the case in 
  many real-world applications. Methods that use different misclassification costs are known as 
  cost-sensitive classifiers. Typical cost-sensitive approaches assume a constant cost for each 
  type of error, in the sense that, the cost depends on the class and is the same among examples 
  \citep{Elkan2001,Kim2012}. 
  
  This class-dependent approach is not realistic in many real-world applications. For 
  example in credit card fraud detection, failing to detect a fraudulent transaction may have an 
  economical impact from a few to thousands of Euros, depending on the particular transaction and 
  card holder \citep{Ngai2011a}. In churn modeling, a model is used for predicting which
  customers are more likely to abandon a service provider. In this context, failing to identify a 
  profitable or unprofitable churner has a significant different economic 
  result~\citep{Verbraken2013}. Similarly, in direct marketing, wrongly predicting that a customer 
  will not accept an offer when in fact he will, may have different financial impact, as not all 
  customers generate the same profit \citep{Zadrozny2003}. Lastly, in credit scoring, accepting 
  loans from bad customers does not have the same economical loss, since customers have different 
  credit lines, therefore, different profits \citep{Verbraken2014}.
  
  Methods that use different misclassification costs are known as cost-sensitive classifiers. In 
  particular, we are interested in methods that are example-dependent cost-sensitive, in the sense 
  that the costs vary among examples and not only among classes \citep{Elkan2001}. However, the 
  literature on example-dependent cost-sensitive methods is limited, mostly because there is a 
  lack of publicly available datasets that fit the problem \citep{MacAodha2013}.
  Example-dependent cost-sensitive classification methods can be grouped according to the step 
  where the costs are introduced into the system. Either the costs are introduced prior to the 
  training of the algorithm, after the training or during training \citep{Wang2013}. In 
  \figurename{ \ref{fig:1:1}}, the different algorithms are grouped according to the stage in a 
  classification system where they are used.
  
  The first set of methods that were proposed to deal with cost-sensitivity consist in 
  re-weighting the training examples based on their costs, either by cost-proportionate 
  rejection-sampling \citep{Zadrozny2003}, or cost-proportionate over-sampling \citep{Elkan2001}.
  The rejection-sampling approach consists in randomly selecting examples from a training set, and 
  accepting each example with probability equal to the normalized misclassification cost of the 
  example. On the other hand, the over-sampling method consists in creating a new set, by making 
  $n$ copies of each example, where $n$ is related to the normalized misclassification cost of the 
  example. These methods however, fail to introduce the example-dependent cost to the training of 
  the different algorithms, and only rely on modifying the prior distribution of the training data.

  The focus of this thesis is to investigate and define different example-dependent cost-sensitive
  classification algorithms, that not only focus on modifying the underling training 
  distribution but also introduce the different real financial costs during the training of the 
  algorithms. We summarize the contributions of this thesis in the following section.
  
  \begin{figure}
  \centering
  \input{figures/src/ch1_fig_algos}
  \caption{Different example-dependent cost-sensitive algorithms grouped according to the 
    stage in a classification system where they are used.}
  \label{fig:1:1}
  \end{figure}
  
\section{Contributions}

This dissertation summarizes several contributions to the field of example-dependent 
cost-sensitive machine learning.
  
\begin{itemize}

\item 
  Binary classification algorithms are normally evaluated using cost-insensitive 
  evaluation measures, such as misclassification rate or \mbox{$F_1Score$}. However, these 
  measures may not be the most appropriate evaluation criteria when  evaluating  
  real-world cost-sensitive problems, because they tacitly assume that 
  misclassification errors carry the same cost. In \citep{CorreaBahnsen2013}, we proposed a new 
  savings example-dependent cost-sensitive evaluation measure. The savings takes into 
  account the actual financial impact of the different misclassification errors.
  
\item 
  Credit card fraud detection is a classical example of a cost-sensitive problem, as the cost of a 
  false negative is significantly different than the cost of a false positive. In 
  \citep{CorreaBahnsen2013,CorreaBahnsen2014}, we discussed the particularities of credit card 
  fraud detection and proposed a financial evaluation measure that takes into account the 
  economical costs associated with credit card fraud. Moreover, when constructing a credit card 
  fraud detection model, it is very important to use those features that allow accurate 
  classification. Typical models only use raw transactional features, such as time, amount, place 
  of the transaction. However, these approaches do not take into account the spending
  behavior of the customer, which is expected to help discover fraud patterns. In 
  \cite{CorreaBahnsen2015c,CorreaBahnsen2015d}, we proposed to create a new set of features based in
  analyzing the periodic behavior of the time of a transaction using the von Mises distribution.
  
\item
  We analyzed and proposed financial evaluation measures for other real-world applications, 
namely, credit card fraud detection \citep{CorreaBahnsen2014b}, churn modeling 
\citep{CorreaBahnsen2015a} and direct marketing~\citep{CorreaBahnsen2014}.

\item
  As part of this work we proposed different example-dependent cost-sensitive
  classification algorithms. In \citep{CorreaBahnsen2013,CorreaBahnsen2014}, we 
  proposed a direct cost approach to make the classification decision based on the expected costs. 
  This method is called Bayes minimum risk, and has been successfully applied to detect credit 
  card fraud. The method consists in quantifying tradeoffs between various decisions using 
  probabilities and the costs that accompany such decisions. 
  
\item
  The Bayes minimum risk method only introduce the costs after the training of an algorithm leaving
  opportunities to investigate the potential impact of algorithms that take into account the real 
  financial example-dependent costs during the training of an algorithm. Therefore, in  
  \citep{CorreaBahnsen2014b}, we proposed a new cost-sensitive logistic regression.
  The   method consists in introducing example-dependent costs into a logistic regression, by 
  changing   the objective function of the model to one that is  cost-sensitive. We then applied a 
  similar approach by introducing the costs in a cost-sensitive decision tree 
  \citep{CorreaBahnsen2015}. The method is based on a new splitting criteria which is 
  cost-sensitive, used during the tree construction. Then, after the tree is fully grown, it is 
  pruned by using a cost-based pruning criteria. 
  
\item
  Based on the cost-sensitive decision tree algorithm, we expand the model by creating a framework 
  for an ensemble of cost-sensitive decision trees \citep{CorreaBahnsen2015b}. This new 
  method, leverage on the advantages of ensemble learning in order to create a more robust model.
  
\item
  The algorithms developed as part of this thesis are publicly available as part of the open-source 
  \textit{CostSensitiveClassification}\footnote{
  \url{https://github.com/albahnsen/CostSensitiveClassification}} library.
  
\end{itemize}

\newpage
\section{Publications}

Publications from this work are:
\bigskip

\begin{itemize}
\item \citep{CorreaBahnsen2013} \textit{Cost Sensitive Credit Card Fraud Detection Using Bayes 
Minimum Risk}, Alejandro Correa Bahnsen,  Aleksandar Stojanovic, Djamila Aouada and Bj\"orn 
Ottersten. In Proceedings of IEEE International Conference on Machine Learning and Applications, 
2013.

\item \citep{CorreaBahnsen2014} \textit{Improving Credit Card Fraud Detection with Calibrated 
Probabilities}, Alejandro Correa Bahnsen, Aleksandar Stojanovic, Djamila Aouada and Bj\"orn 
Ottersten. In Proceedings of SIAM International Conference on Data Mining, 2014.

\item \citep{CorreaBahnsen2014b} \textit{Example-Dependent Cost-Sensitive Logistic Regression for 
Credit Scoring}, Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten.
In Proceedings of IEEE International Conference on Machine Learning and Applications, 2014.

\item \citep{CorreaBahnsen2015} \textit{Example-Dependent Cost-Sensitive Decision Trees},
Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten.
In Expert Systems with Applications, 42(19):6609-6619, 2015.

\item \citep{CorreaBahnsen2015a} \textit{A novel cost-sensitive framework for customer churn 
predictive modeling}, Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten.
In Decision Analytics, 2(1):5, 2015.

\item \citep{CorreaBahnsen2015b} \textit{Ensembles of Example-Dependent Cost-Sensitive Decision 
Trees}, Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten.
Submitted to IEEE Transactions on Knowledge and Data Engineering, 2015.

\item \citep{CorreaBahnsen2015c} \textit{Feature Engineering in Credit Card Fraud Detection},
Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten. Submitted to Expert Systems with 
Applications, 2015.

\newpage
\item \citep{CorreaBahnsen2015d} \textit{Detecting Credit Card Fraud using Periodic Features}, 
Alejandro Correa Bahnsen, Djamila Aouada and Bj\"orn Ottersten.
Submitted to the IEEE International Conference on Machine Learning and Applications, 2015.


\end{itemize}


\section{Outline}

\partname{~\textsc{\ref{part:1}}} of this manuscript is focused on giving the general concepts of 
classification and cost-sensitive classification. In particular, in \chaptername{~\ref{ch:2}}, we 
give a background on classification. Then, in \chaptername{~\ref{ch:3}}, we present 
the cost-sensitive problem and define the difference between 
cost-insensitive, class-dependent cost-sensitive and example-dependent cost-sensitive 
classification problems. Lastly, we give an introduction of the different evaluation measures 
that are used throughout this thesis.

\partname{~\textsc{\ref{part:2}}} is dedicated to explaining the particularities of the four 
real-world classification problems that are the focus of this thesis, in particular, credit card 
fraud detection, credit scoring, churn modeling and direct marketing. In general, we show why each 
of the applications is example-dependent cost-sensitive, and we elaborate a framework for the 
analysis of each problem. This part is organized in two chapters. First, in 
\chaptername{~\ref{ch:4}}, we discuss the applications within financial risk management. 
Second, in \chaptername{~\ref{ch:5}}, we analyze marketing analytics applications.

\partname{~\textsc{\ref{part:3}}} is focused on introducing our proposed example-dependent 
cost-sensitive methods. First, in \chaptername{~\ref{ch:6}}, we present the Bayes minimum risk 
method. Then, we introduce the cost-sensitive logistic regression algorithm in 
\chaptername{~\ref{ch:7}}. Afterwards, in \chaptername{~\ref{ch:8}}, we show and discuss the 
cost-sensitive decision trees algorithm. Lastly, in \chaptername{~\ref{ch:9}}, we present our 
framework for ensembles of cost-sensitive decision trees. 

\chaptername{ \ref{ch:10}} concludes the thesis, and elaborates on possible lines for future 
research. Lastly, in the Appendix~\ref{ch:A}, we present the library \mbox{\textit{CostCla}}  
developed as part of the thesis. This library is an open-source implementation of all the 
algorithms covered in this manuscript.
