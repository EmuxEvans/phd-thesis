\chapter{Background}

\begin{remark}{Outline}
In this chapter, we introduce classifications models from a machine learning perspective. 
Moreover, we present the example-dependent cost-sensitive framework that is the basis of the 
thesis. In Section~\ref{sec:2:classification}, we give a self contain introduction to 
classification, including the most-common algorithms, and the main methods for evaluating the 
performance of the different algorithm. Then, in Section~\ref{sec:2:cs}, we present the general 
framwork of cost-sensitive classification. Within this Section, we first introduce a method for 
defining the type of cost-sensitivity of a problem. Afterwards, we present the different 
cost-sensitive performance evaluation measures. Lastly, we show the state-of-the-art 
example-dependent cost-sensitive methods cost-proportionate rejection-sampling and 
cost-proportionate over-sampling.
\end{remark}


\section{Basics on classification}
\label{sec:2:classification}

In machine learning, classification refers to the attempt of identifying to which of a set of 
classes a new example belongs, based on learning from examples whose class membership is known. 
The most important point about classification is that for each example only one know class is 
possible, making this a discrete problem. 

A classification, task begins with a training set in which the class of a set of examples is know. 
For example, a classification model that predicts credit card fraud, is developed by analyzing 
many observed credit transactions over a period of time. The class in this case is a variable which 
indicates for each example whether or not the transaction was o not a fraud. Also, the predictors 
or features, are the transaction attributes like place, amount and time of the transaction.

Then, during the training process, a classification algorithm finds the patterns and relationships 
between the values of the features and the values of the target class. Different algorithms use 
different methods and techniques to estimate the relationships. Afterwards, these relationships are 
summarized in a model that is able to make predictions on new sets of data.

Formally, a classification algorithm deals with the problem	of predicting the class $y_i$ of a 
set $\mathcal{S}$ of examples or instances, given their $k$ features \mbox{$\mathbf{x}_i \in 
\mathbb{R}^k$}. The objective is to construct a function $f(\cal{S})$ that makes a prediction 
$c_i$ of the class of each of the $N$ examples using its feature vector $\mathbf{x}_i$.
\label{ntn:ch2:1}
Moreover, some algorithm allow to not only know the prediction, but also the confidence of it, in 
the form of the probability of belonging to the positive class $\hat p_i$. The way for going from 
$\hat p_i$ to $c_i$, is simply by defining a probability threshold $t$, and applying the following 
formula
\begin{equation}\label{eq_pred}
  c_i = 
  \begin{cases}
    \phantom{-}0 \phantom{-} \mbox{if} \phantom{-} \hat p_i \le t\\
    \phantom{-}1 \phantom{-}\mbox{otherwise,}
  \end{cases}
\end{equation}
Usually, $t=\frac{1}{2}$ \citep{Hastie2009}, as an example with probability of being positive 
higher than 50\% is classified as positive, and negative otherwise. If $t \ne \frac{1}{2}$, the 
function that generates the predicted class labels $\mathbf{c}$ is refers as $f^t$.

\begin{figure}
	\centering
	\input{figures/src/ch2_fig_classification_process}
  \caption{Classification process}
  \label{fig:2:1}
\end{figure}

In \figurename{ \ref{fig:2:1}}, the process of a training and using a classification algorithm is 
summarized. First, during the training phase, using a training set $\mathcal{S}_{train}$, a 
algorithm is train to predict $\mathbf{y}$. Then the algorithm is used to make estimate the class 
$\mathbf{c}$ of a set of testing examples $\mathcal{S}_{test}$.

\begin{figure}[!t]
\centering
\subfloat[]{\includegraphics{ch2_fig_2a}\label{fig:2:2a}}
\\
\subfloat[]{\includegraphics{ch2_fig_2b}\label{fig:2:2b}}
\caption{Example of a classification algorithm. Using a set of examples from two classes, a 
	classification algorithm is learn in order to separate between the positives and the negatives. }
\label{fig:2:2}
\end{figure} 

There exists several algorithms that can be used for classification tasks. In general a 
classification algorithm is learn with the objective of finding patterns that separate between the 
different classes \citep{Hastie2009}. In order to clarify this intuition, in \figurename{ 
\ref{fig:2:2}} an example of a classification algorithm is shown. Lets suppose a set of examples as 
shown in \figurename{ \ref{fig:2:2a}}.  Where the red points represents the positive examples and 
the blue the negative examples. The objective of a classifier is to find the best way to separate 
between the positive and negatives examples. In \figurename{ \ref{fig:2:2b}}, the output of a 
classifier learned using the set of examples is shown. It is observed that this classifier is able 
to separate almost all the examples using a linear classifier. 

However, not all examples are correctly classified. In particular there are four negative examples 
that were predicted as positive, and five positive examples that were predicted as negative. In the 
next section, we present the standard methods for evaluating the performance of a classification 
algorithm.

\begin{remark}{Classification examples}
Classification algorithms are widely used across a variety of domains. For example in the 
medical field, models have been used for making predictions about tumors, probability 
of a disease, selecting the right drug for a particular patient, and estimating the probability of 
relapsing, among others \citep{Herland2014}. In the financial sector, classification models have 
been successfully applied for fraud detection, credit scoring, portfolio management and algorithmic 
trading. Also, in marketing, several models are being currently used for churn modeling, customer 
targeting, behavior prediction and direct marketing \citep{Baesens2014}. Additionally, in many 
other emerging applications such as terrorism prevention, malware detection, computer security, 
energy consumption prediction, spam classification, and others \citep{Kriegel2007}.
\end{remark}


\subsection{Performance measures}

When evaluating the performance of a classification algorithm, the first thing to do is to check 
the number of examples that were misclassified. Since the the true class of the training examples 
is known. Therefore, evaluating the error of a model is as simply as counting the number of times 
an example is misclassified and divide it by the number of examples
\begin{equation}\label{eqn:ch2:error}
Err(f({\cal S})) = \frac{1}{N}  \sum_{i=1}^N \mathbf{1}_{y_i}(c_i),
\end{equation}
where $\mathbf{1}_c(z)$ is an indicator function that takes the value of one if $z \in c$ and 
zero if $z \notin c$. Moreover the accuracy is defined as the percentage of times the algorithm 
made the correct prediction
\begin{equation}\label{eqn:2:accuracy}
Acc(f({\cal S})) = 1- Err(f({\cal S})).
\end{equation}

However, just knowing these statistics is not enough to make decisions, as in many applications is 
important to know where the errors are coming from. In particular, the misclassified examples may 
belong only to one class, which may give interesting insights about the problem. A way to observe 
the different errors is by looking to the confusion matrix, as shown in 
\mbox{\tablename{~\ref{tab:2:1}}}. Afterwards, using the cost matrix several statistics are 
extracted. In particular:
  \begin{flalign}
    &Recall = \frac{TP}{TP+FN} &\\
    &Precision = \frac{TP}{TP+FP}& \\
    &F_1Score = 2\frac{Precision \cdot Recall}{Precision + Recall}&
  \end{flalign}
  
	\begin{table}[!t]
		\centering
		\footnotesize
    \begin{tabular}{c|c|c}
      \multicolumn{3}{c}{}\\
			\multicolumn{1}{c|}{}  & Actual Positive& Actual Negative \\
			\multicolumn{1}{c|}{} & $y=1$& $y=0$ \\
			\hline
			Predicted Positive 		& \multirow{ 2}{*}{True Positive ($TP$)} & \multirow{ 
			2}{*}{False Positive ($FP$)} \\
			$c=1$ & &\\
			\hline
			Predicted Negative  	& \multirow{ 2}{*}{False Negative ($FN$)} & \multirow{ 
			2}{*}{True Negative ($TN$)} \\
			$c=0$ & &\\
		\end{tabular}
		\caption{Classification confusion matrix}
		\label{tab:2:1}
  \end{table}  
 
  As an illustrative example, the different statistics are calculated for the toy example presented 
  in Section~\ref{sec:2:classification}. First, the confusion matrix is calculated as 
  follows:
  \begin{center}
    \footnotesize
  \begin{tabular}{c|c|c}
    \multicolumn{1}{c|}{}  & Actual Positive& Actual Negative \\
    \multicolumn{1}{c|}{} & $y=1$& $y=0$ \\
    \hline
    Predicted Positive    & \multirow{ 2}{*}{36} & \multirow{ 
    2}{*}{4} \\
    $c=1$ & &\\
    \hline
    Predicted Negative    & \multirow{ 2}{*}{5} & \multirow{ 
    2}{*}{68} \\
    $c=0$ & &\\
  \end{tabular}
  \end{center}
  Then using the confusion matrix, the statistics are calculated,
 	\begin{itemize}
  	\item Error = $\frac{4+9}{36+4+9+68}=11.11\%$
		\item Recall = $\frac{TP}{TP+FN}=87.8\%$
		\item Precision = $\frac{TP}{TP+FP}=90\%$
		\item $F_1Score = 2\frac{Precision \cdot Recall}{Precision + Recall}=88.8\%$
	\end{itemize}
	
\begin{figure}[t!]
	\centering
	\includegraphics{ch2_fig_3}
	\caption{Example of a classification algorithm. Using a set of examples from two classes, a 
	classification algorithm is learn in order to separate between the positives and the negatives. }
	\label{fig:ch2:3}
\end{figure}

There are however, several instances that are misclassified, thats because the simple linear 
classifier that were used in this example may not be good enough to separate between the positive 
and negative classes. In order to make a comparison, using the same example toy set, a new 
algorithm is learn. This time the algorithm made the correct prediction more often as shown in 
\figurename{ \ref{fig:ch2:3}}. Afterwards, the confusion matrix and the different statistics are 
calculated as follows:
\begin{center}
		\footnotesize
    \begin{tabular}{c|c|c}
			\multicolumn{1}{c|}{}  & Actual Positive& Actual Negative \\
			\multicolumn{1}{c|}{} & $y=1$& $y=0$ \\
			\hline
			Predicted Positive 		& \multirow{ 2}{*}{37} & \multirow{ 
			2}{*}{2} \\
			$c=1$ & &\\
			\hline
			Predicted Negative  	& \multirow{ 2}{*}{4} & \multirow{ 
			2}{*}{70} \\
			$c=0$ & &\\
		\end{tabular}
\end{center}
  \begin{itemize}
    \item Error = $\frac{4+9}{36+4+9+68}=5.3\%$
    \item Recall = $\frac{TP}{TP+FN}=90.2\%$
    \item Precision = $\frac{TP}{TP+FP}=94.9\%$
    \item $F_1Score = 2\frac{Precision \cdot Recall}{Precision + Recall}=92.5\%$
  \end{itemize}
It is observed that in this case the FP are reduced more than the FN, this leads to a higher 
increase in precision  than in recall. There is not a single rule regarding which one is more 
important to increase, it depends on the applications. For example in applications with a high 
false negative cost such as failing to identify a tumor in a medical exam, the recall should be the 
priority, even if that implies having a significant number of false positives. On the other hand, 
In applications such as spam detection, predicting a normal email as spam, may have a big impact to 
the customer, therefore, in this example is better to allow some false negatives and focus on the 
false positives.

It is not always straightforward  to define the define the right tradeoff between false positives 
and false negatives. The best approximation to solve that, is to focus on the actual costs incurred 
by the different decisions, this is usually solved using cost-sensitive classification methods. A 
deep analysis of these methods is described in the following Section.

%\subsection{Families of binary classifiers}
% \subsection{Linear models}
% \subsection{Decision tree models}
% \subsection{Neural networks}
% \subsection{Suport vector machines}
% \subsection{Ensemble based classifiers}
% \section{Sampling}
% \subsection{Under/over sampling}
% \subsection{SMOTE}


\section{Cost-sensitive classification}
\label{sec:2:cs}

  Classification, in the context of machine learning, deals with the problem of predicting the class
  of a set of examples given their features. Traditionally, classification methods aim at 
  minimizing the misclassification of examples, in which an example is misclassified if the 
  predicted class is different from the true class. Such a traditional framework assumes that all 
  misclassification errors carry the same cost. This is not the case in many real-world 
  applications. Methods that use different misclassification costs are known as cost-sensitive 
  classifiers. Typical cost-sensitive approaches assume a constant cost for each type of error, in 
  the sense that, the cost depends on the class and is the same among examples  
  \citep{Elkan2001,Kim2012}. 
  Although, this class-dependent approach is not realistic in many real-world applications.
  
  \begin{remark}{Real-world cost sensitive applications}
  For example in credit card fraud detection, failing to detect a fraudulent transaction may have 
  an economical impact from a few to thousands of Euros, depending on the particular transaction 
  and card holder \citep{Sahin2013}. In churn modeling, a model is used for predicting which
  customers are more likely to abandon a service provider. In this context, failing to identify a 
  profitable or unprofitable churner have a significant different financial impact 
  \citep{Glady2009}. Similarly, in direct marketing, wrongly predicting that a customer will not 
  accept an offer when in fact he will, has a different impact than the other way around 
  \citep{Zadrozny2003}. Also in credit scoring, where declining good customers has a non constant 
  impact since not all  customers generate the same profit \citep{Verbraken2014}. Lastly, in the 
  case of intrusion   detection, classifying a benign connection as malicious have a different cost 
  than when a   malicious connection is accepted \citep{Ma2011}.
  \end{remark}

  In order to deal with these specific types of cost-sensitive problems, called example-dependent
  cost-sensitive, some methods have been proposed recently. However, the literature on 
  example-dependent cost-sensitive methods is limited, mostly because there is a lack of publicly 
  available datasets that fit the problem \citep{MacAodha2013}. Standard solutions consist in 
  modifying the training set by re-weighting the examples proportionately to the misclassification 
  costs \citep{Elkan2001,Zadrozny2003}.

  
\subsection{Binary classification cost characteristic}
  In classification problems with two classes $y_i \in \{0,1\}$, the objective is to learn or 
  predict to which class $c_i \in \{0,1\}$ a given example $i$ belongs based on its $k$ features 
  $\mathbf{x}_i=[x^1_i, x^2_i,...,x^k_i]$. In this context, classification costs can be 
  represented using a 2x2 cost matrix \citep{Elkan2001}, that introduces the costs 
  associated with   two types of correct   classification, true positives ($C_{TP_i}$), true 
  negatives ($C_{TN_i}$),   and the two  types of   misclassification errors, false positives 
  ($C_{FP_i}$), false negatives   ($C_{FN_i}$), as   defined in \tablename{ 
  \ref{tab:2:cost_matrix}}.
  \label{ntn:ch2:2}
  
  \begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c|c|c}
      \multicolumn{1}{c|}{}  & Actual Positive& Actual Negative \\
      \multicolumn{1}{c|}{} & $y_i=1$& $y_i=0$ \\
      \hline
      Predicted Positive    & \multirow{ 2}{*}{$C_{TP_i}$} & \multirow{ 2}{*}{$C_{FP_i}$} \\
      $c_i=1$ & &\\
      \hline
      Predicted Negative    & \multirow{ 2}{*}{$C_{FN_i}$} & \multirow{ 2}{*}{$C_{TN_i}$} \\
      $c_i=0$ & &\\
    \end{tabular}
    \caption{Classification cost matrix}
    \label{tab:2:cost_matrix}
  \end{table}  

  Conceptually, the cost of correct classification should always be lower than the cost of 
  misclassification. These are referred to as the ``reasonableness`` conditions \citep{Elkan2001}, 
  and are defined as  $C_{FP_i} > C_{TN_i}$ and $C_{FN_i} > C_{TP_i}$.
  Taking into account the ``reasonableness`` conditions, a simpler cost matrix 
  with only one degree of freedom has been defined in \citep{Elkan2001},
  by scaling and shifting the initial cost matrix, resulting in:
  \begin{center}
    \footnotesize
    \begin{tabular}{c|c}
    \multirow{ 2}{*}{Negative} & \multirow{ 
    2}{*}{$C^*_{FN_i}=\frac{(C_{FN_i}-C_{TN_i})}{(C_{FP_i}-C_{TN_i})}$} \\
    \\
    \hline
    \multirow{ 2}{*}{Positive} & \multirow{ 
    2}{*}{$C^*_{TP_i}=\frac{(C_{TP_i}-C_{TN_i})}{(C_{FP_i}-C_{TN_i})}$} \\
    \\ 
    \end{tabular}
  \end{center}

  A classification problem is said to be cost-insensitive if costs of both errors are equal. It 
  is class-dependent cost-sensitive if the costs are different but constant. Finally we talk 
  about an example-dependent cost-sensitive classification problem if the cost matrix is not 
  constant for all the examples.

  However, the definition above is not general enough. There are many cases when the cost matrix 
  is not constant and still the problem is cost-insensitive or class-dependent cost-sensitive. 
  For example, if the costs of correct classification are zero, $C_{TP_i}=C_{TN_i}=0$, 
  and the costs of misclassification are $C_{FP_i}=a_0\cdot z_i$ and $C_{FN_i}=a_1\cdot z_i$,
  where $a_0$, $a_1$, are constant and $z_i$ a random variable. This is an example of a cost 
  matrix that is not constant. However, $C^*_{FN_i}$ and $C^*_{TP_i}$ are constant, i.e. 
  $C^*_{FN_i}=(a_1\cdot z_i)/(a_0\cdot z_i)=a_1/a_0$ and $C^*_{TP_i}=0$ $\forall i$. In 
  this case the problem is cost-insensitive if $a_0=a_1$, or class-dependent cost-sensitive if 
  $a_0 \ne a_1$, even given the fact that the cost matrix is not constant.

  Nevertheless, using only the simpler cost matrix is not enough to define when a problem is 
  example-dependent cost-sensitive. To achieve this, we proposed in \citep{CorreaBahnsen2015}, 
  the classification problem cost characteristic as:
  \begin{equation}
    b_i = C^*_{FN_i}-C^*_{TP_i},
  \end{equation}
  and define its mean and standard deviation as $\mu_b$ and $\sigma_b$, respectively.

  Using $\mu_b$ and $\sigma_b$, we analyze different binary classification problems. In the case 
  of a cost-insensitive classification problem, for every example $i$ \mbox{$C_{FP_i}=C_{FN_i}$}
  and $C_{TP_i}=C_{TN_i}$, leading to $b_i=1$ $\forall i$ or more generally $\mu_b=1$ and 
  $\sigma_b=0$. For class-dependent cost-sensitive problems, the costs are not equal but 
  constants \mbox{$C_{FP_i}\ne C_{FN_i}$} or \mbox{$C_{TP_i}\ne C_{TN_i}$}, leading to $b_i \ne 
  1$ $\forall i$, or $\mu_b \ne 1$ and $\sigma_b=0$. Lastly, in the case of example-dependent 
  cost-sensitive problems, the cost difference is non constant or $\sigma_b \ne 0$.

  In summary a binary classification problem is defined according to the following conditions:
  \begin{center}
    \footnotesize
    \begin{tabular}{c | c | l}
      $\mu_b$ & $\sigma_b$ & Type of classification problem \\
      \hline 
      && \\
      $1$ &  $0$ & cost-insensitive \\ &&\\
      $\ne 1$ & $0$ & class-dependent cost-sensitive \\ &&\\
      & $\ne 0$ & example-dependent cost-sensitive \\ 
    \end{tabular}
  \end{center}
  

\subsection{Example-dependent evaluation measures}
\label{sec:2:csmeasures}

  Common cost-insensitive evaluation measures such as misclassification rate or \mbox{$F_1Score$}, 
  assume the same cost for the different misclassification errors. Using these measures is not 
  suitable for example-dependent cost-sensitive binary classification problems. Indeed, two 
  classifiers with equal misclassification rate but different numbers of false positives and 
  false negatives do not have the same impact on cost since \mbox{$C_{FP_i}\ne C_{FN_i}$};
  therefore there is a need for a measure that takes into account the actual costs 
  $\{C_{TP_i},C_{FP_i},C_{FN_i},C_{TN_i}\}$ of each example $i$, as introduced in the previous 
  Section.
  \label{ntn:ch2:3}

  Let $\mathcal{S}$ be a set of $N$ examples $i$, $N=\vert S \vert$, where each example is 
  represented by  the augmented feature vector $\mathbf{x}_i^*=[\mathbf{x}_i, 
  C_{TP_i},C_{FP_i},C_{FN_i},C_{TN_i}]$  and labeled using the class   label $y_i   \in \{0,1\}$. 
  A classifier $f$ which generates the   predicted label $c_i$ for each   element $i$ is trained  
  using the set $\mathcal{S}$. Then the cost of   using $f$ on $\mathcal{S}$ is calculated by
  \begin{align}\label{eq:cost}
    Cost(f(S)) = \sum_{i=1}^{N} & {\bigg( y_i(c_i C_{TP_i} + (1-c_i)C_{FN_i})} + \\
    & {(1-y_i)(c_i C_{FP_i} + (1-c_i)C_{TN_i}) \bigg)}.
  \end{align}

  However, the total cost may not be easy to interpret. In \citep{Whitrow2008}, a 
  \textit{normalized} cost measure was proposed, by dividing the total cost by the theoretical 
  maximum cost, which is the cost of misclassifying every example. The \textit{normalized} cost is 
  calculated using
  \begin{align}\label{eq:ncost}
    Cost_n(f(\mathcal{S})) = \frac{Cost(f(\mathcal{S}))}
    {\sum_{i=1}^N C_{FN_i} \cdot \mathbf{1}_0(y_i) 
    +  C_{FP_i} \cdot \mathbf{1}_1(y_i)  }.
  \end{align} 

  We proposed similar approach in \citep{CorreaBahnsen2014b}, where the savings of using an 
  algorithm  are defined as the cost of the algorithm versus the cost of using no algorithm at all. 
  To do that, the cost of the costless class is defined as 
  \begin{equation}
    Cost_l(\mathcal{S}) = \min \{Cost(f_0(\mathcal{S})), Cost(f_1(\mathcal{S}))\},
  \end{equation}
  where 
  \begin{equation}\label{eq:f_a}
    f_a(\mathcal{S}) = \mathbf{a}, \text{ with } a\in \{0,1\}.
  \end{equation}

  The cost improvement can be expressed as the cost savings as compared with $Cost_l(\mathcal{S})$. 
  \begin{equation}\label{eq:savings}
    Savings(f(\mathcal{S})) = \frac{ Cost_l(\mathcal{S}) - Cost(f(\mathcal{S}))}
    {Cost_l(\mathcal{S})}.
  \end{equation} 


\subsection{State-of-the-art methods}

  As mentioned earlier, taking into account the different costs associated with each example, 
  some methods have been proposed to make classifiers example-dependent cost-sensitive. These 
  methods may be grouped in two categories. Methods based on changing the class distribution of 
  the training data, which are known as cost-proportionate sampling methods; and direct cost 
  methods \citep{Wang2013}.

  A standard method to introduce example-dependent costs into classification algorithms is to 
  re-weight the training examples based on their costs, either by cost-proportionate 
  rejection-sampling \citep{Zadrozny2003}, or over-sampling \citep{Elkan2001}. The 
  rejection-sampling approach consists in selecting a random subset $\mathcal{S}_{r}$  by 
  randomly  selecting examples from $\mathcal{S}$, and accepting each example $i$ with 
  probability $w_i/ \max\limits_{1,\dots, N}\{w_i\}$, where $w_i$ is defined as the expected 
  misclassification error of example $i$:
  \begin{equation}\label{eq_pred1}
    w_i = y_i\cdot C_{FN_i}+(1-y_i)\cdot C_{FP_i}.
  \end{equation}
  Lastly, the over-sampling method consists in creating a new set $\mathcal{S}_{o}$, by making 
  $w_i$ copies of each example $i$. However, cost-proportionate over-sampling increases the 
  training  since $\vert \mathcal{S}_{o}\vert >> \vert \mathcal{S} \vert$, and it also may result 
  in over-fitting  \citep{Drummond2003}. Furthermore, none of these methods uses the full cost 
  matrix but only the  misclassification costs.

  The second approach, consists in using the predicted probability $\hat p_i$, estimated using a 
  given classifier $f$, and   modify the threshold $t$  such that the savings are maximized. 
  This method is called cost-sensitive thresholding \citep{Sheng2006}. The idea behind this approach 
  is to adaptively modify the probability threshold of an algorithm $f_t$ in order to maximized the 
  savings of the algorithm on a given set $Savings(f^t(\mathcal{S}))$. The threshold is calculated 
  using the following equation
  \begin{equation}
   t_{thresholding} = \argmax_t Savings(f^t(\mathcal{S})).
  \end{equation}

  