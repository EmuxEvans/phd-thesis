\chapter{Bayes minimum risk}\label{ch:6}

\begin{remark}{Outline}
In this chapter, we present our previously proposed Bayes minimum risk algorithm. The method 
consists in quantifying tradeoffs between various decisions using probabilities and the costs that 
accompany such decisions. First, in Section~\ref{sec:6:bmr}, we present the Bayes minimum risk 
algorithm. Then, in Section~\ref{sec:6:prob}, we discuss the impact of calibrating the probabilities 
in the model. Finally, in Section~\ref{sec:6:experiments}, we compare the results of the proposed 
algorithm, against state-of-the-art methods, using the five real-world cost-sensitive databases.
\end{remark}


\section{Bayes minimum risk model}
\label{sec:6:bmr}

% % As decribed in Section \ref{sec:3:cs}, state-of-the-art example dependent techniques, only 
% introduce the cost by modifying the training set. In \citep{CorreaBahnsen2013,CorreaBahnsen2014}, 
% we proposed a cost-sensitive model called Bayes minimum risk classifier ($BMR$).  

As defined in \citep{Ghosh2006}, the BMR classifier is a decision model based on quantifying 
tradeoffs between various decisions using probabilities and the costs that accompany such decisions. 
This is done in a way that for each example the expected losses are minimized. In  what follows, we 
consider the probability estimates $\hat p_i$ as known, regardless of the algorithm used to 
calculate them.  The risk that accompanies each decision is calculated using the cost matrix 
as described in \tablename{ \ref{tab:3:cost_matrix}}. In the specific framework of binary 
classification, the risk of predicting the example $i$ as negative is 
\begin{equation}
  R(c_i=0|\mathbf{x}_i)=C_{TN_i}(1-\hat p_i)+C_{FN_i} \cdot \hat p_i, 
\end{equation}
and
\begin{equation}
  R(c_i=1|\mathbf{x}_i)=C_{TP_i} \cdot \hat p_i + C_{FP_i}(1- \hat p_i), 
\end{equation}
is the risk when predicting the example as positive, where $\hat p_i$ is the estimated positive 
probability for example $i$. Subsequently, if 
\begin{equation}
  R(c_i=0|\mathbf{x}_i) \le R(c_i=1|\mathbf{x}_i), 
\end{equation}
then  the example $i$ is classified as negative. This means that the risk associated with the 
decision $c_i$ is lower than the risk associated with classifying it as positive. 

\begin{remark}{Characteristics of probability estimates}
When using the output of a binary classifier as a basis for decision making, there is a 
need for a probability that not only separates well between positive and negative examples, but 
that also assesses the real probability of the event \citep{cohen2004}.
\end{remark}


\section{Calibration of probabilities}
\label{sec:6:prob}

In this section, two methods for calibrating probabilities are explained. First, the method proposed 
in \citep{Elkan2001} to adjust the probabilities based on the   difference in bad rates  between the 
training and testing datasets.  Then, the method proposed in \cite{Hernandez-Orallo2012}, in which 
calibrated probabilities are extracted after modifying the ROC curve using the ROC convex hull 
methodology, is described.
 
\subsection{Calibration due to a change in base rates}

One of the reasons why a probability may not be calibrated is because the algorithm is trained 
using a dataset with a different base (or positive) rate than the one on the evaluation dataset.  
This is something common in machine learning since using under-sampling or over-sampling is a 
typical method to solve problems such as class imbalance and cost sensitivity \citep{Hulse2007}.
  
In order to solve this and find probabilities that are calibrated, in \citep{Elkan2001} a formula  
that corrects the probabilities based on the difference of the base rates is proposed.  The 
objective is using $\hat p$ which was estimated using a population with base rate $\pi_1$,
to find $\hat p'$ for the real population which has a base rate $\pi_1'$. A solution for $\hat p'$ 
is given as follows:
\begin{equation}
  \hat p'=\pi_1' \frac{\hat p - \hat p \pi_1}{\pi_1- \pi_1 \hat p +\pi_1' \hat p - \pi_1 \pi_1'}.
\end{equation}

% Nevertheless, a strong assumption is made by taking: $P'(x|j=1)=P(x|j=1)$} and 
% \mbox{$P'(x|j=0)=P(x|j=0)$}, meaning that there
%   is no change in the example probability within the positive and negative
%   subpopulations density functions.
  
\subsection{Calibrated using the ROC convex hull}

In order to illustrate the ROC convex hull approach proposed in \citep{Hernandez-Orallo2012},
let us consider the set of probabilities given in \figurename{~\ref{tab:6:example_prob}}.
Their corresponding  ROC curve of that set of probabilities is shown in 
\figurename{~\ref{fig:6:ROC_1}}.  It can be seen that this set of probabilities is not calibrated, 
since at 0.1 there is a positive example  followed by 2 negative examples. That inconsistency is 
represented in the ROC curve as a non convex segment  over the curve.
  
\begin{figure}[!t]
\hskip 0.5cm
\hbox{
  \vtop{
    \hbox{
      \subfloat[Set of probabilities and their respective class label]{
   \footnotesize
	\begin{tabular}{cc}
	\hline
	Probability & Label\\
	\hline
	0.0&	0\\
	0.1&	1\\
	0.2&	0\\
	0.3&	0\\
	0.4&	1\\
	0.5&	0\\
	0.6&	1\\
	0.7&	1\\
	0.8&	0\\
	0.9&	1\\
	1.0&	1\\
	\hline
	\end{tabular}\label{tab:6:example_prob}
      }
    }
  }\hskip 0.5cm
  \vtop{\vskip -2.5cm
    \subfloat[ROC curve of the set of probabilities]{
      \includegraphics[scale=0.5]{ch6_fig2}\label{fig:6:ROC_1}
    }%
  }%
}
\vskip 1.5cm

\hbox{ 
  \vtop{ \vskip 0.5cm
    \hbox{
      \subfloat[Convex hull of the ROC curve]{
	\includegraphics[scale=0.5]{ch6_fig1}\label{fig:6:ROC_2}
      }
    }
  }\hskip 1cm
  \vtop{ \vskip -1cm
    \subfloat[Calibrated probabilities]{
   \footnotesize
	\begin{tabular}{cc}
	\hline
	Prob & Cal Prob\\
	\hline
	0.0&	0\\
	0.1&	0.333\\
	0.2&	0.333\\
	0.3&	0.333\\
	0.4&	0.5\\
	0.5&	0.5\\
	0.6&	0.666\\
	0.7&	0.666\\
	0.8&	0.666\\
	0.9&	1\\
	1.0&	1\\
	\hline
	\end{tabular}\label{fig:6:cal_prob} 
    }
  }
}
\caption{Estimation of calibrated probabilities using the ROC convex hull.}\label{fig:6:rocch}
\end{figure}

In order to obtain a set of calibrated probabilities, first the ROC curve must be modified in 
order to be convex. The way to do that, is to find the convex \mbox{hull 
\citep{Hernandez-Orallo2012}}, in order to obtain the minimal convex set containing the different 
points of the ROC curve. In \figurename{~\ref{fig:6:ROC_2}}, the convex hull algorithm is applied 
to the previously evaluated ROC curve. It is shown that the new curve is convex, and includes all 
the points of the previous ROC curve.

Now that there is a new convex ROC curve or ROCCH, the calibrated probabilities can be extracted 
as shown in  \figurename{~\ref{fig:6:cal_prob}}. The procedure to extract the new probabilities is 
to first group the probabilities according to the points in the ROCCH curve, and then make the 
calibrated  probabilities be the slope of the ROCCH for each group.
  
   
\section{Experiments}
\label{sec:6:experiments}

In this section, first the sumarized the datasets used for the experiments. Then, the 
partitioning of the dataset and the algorithms used are shown. Finally, we present the results.

\subsection{Experimental setup}

For the experiments we used five datasets from four different real world example-dependent 
cost-sensitive problems: Credit card fraud detection (see Section~\ref{sec:4:fraud}), credit 
scoring (see Section~\ref{sec:4:creditscoring}), churn modeling (see Section~\ref{sec:3:churn}) and 
direct marketing (see Section~\ref{sec:3:directmarketing}).

For each dataset we used a pre-define a cost matrix that we previously proposed in different 
publications. Additionally,  For each database, 3 different datasets are extracted: train-
ing, validation and testing. Each one containing 50\%, 25\% and 25\% of the transactions, 
respectively. Afterwards, because classification algorithms suffer when the label distribution is 
skewed towards one of the classes \citep{Hastie2009}, an under-sampling of the positive examples is 
made, in order to have a balanced class distribution. Moreover, we perform the cost-proportionate 
rejection-sampling and cost proportionate over-sampling procedures, that we previously described in 
Section~\ref{sec:3:costsampling}. \tablename{~\ref{tab:6:databases}}, summarizes the 
different datasets. It is important to note that the sampling procedures were only applied to the 
training dataset since the validation and test datasets must reflect the real distribution.

\begin{table}
  \centering
  \footnotesize
  \begin{tabular}{l l c c c } %sum 7.7
    \hline
    \textbf{Database}& \textbf{Set}&  \textbf{\# Obs} ($N$) & \textbf{\%Pos} ($\pi_1$)& 
    \textbf{Cost} \\
    \hline
    Fraud &Total&236,735&1.50&895,154\\
    Detection&  Training ($t$)&94,599&1.51&358,078\\
    &Under-sampled ($u$)&2,828&50.42&358,078\\
    &Rejection-sampled ($r$)&94,522&1.43&357,927\\
    &Over-sampled ($o$)&189,115&1.46&716,006\\
    &Validation&70,910&1.53&274,910\\
    &Testing&71,226&1.45&262,167\\
    \hline
    Credit  & Total&112,915&6.74&83,740,181\\
    Scoring 1 & Training ($t$)&45,264&6.75&33,360,130\\
    &Under-sampled ($u$)&6,038&50.58&33,360,130\\
    &Rejection-sampled ($r$)&5,271&43.81&29,009,564\\
    &Over-sampled ($o$)&66,123&36.16&296,515,655\\
    &Validation&33,919&6.68&24,786,997\\
    &Testing&33,732&6.81&25,593,055\\
    \hline
    Credit &Total&38,969&19.88&3,117,960\\
    Scoring 2&Training ($t$)&15,353&19.97&1,221,174\\
    &Under-sampled ($u$)&6,188&49.56&1,221,174\\
    &Rejection-sampled ($r$)&2,776&35.77&631,595\\
    &Over-sampled ($o$)&33,805&33.93&6,798,282\\
    &Validation&11,833&20.36&991,795\\
    &Testing&11,783&19.30&904,991\\
    \hline
    Churn&Total&9,410&4.83&580,884\\
    Modeling&Training ($t$)&3,758&5.05&244,542\\
    &Under-sampled ($u$) &374&50.80&244,542\\
    &Rejection-sampled ($r$)&428&41.35&431,428\\
    &Over-sampled ($o$) &5,767&31.24&2,350,285\\
    &Validation&2,824&4.77&174,171\\
    &Testing&2,825&4.42&162,171\\
    \hline
    Direct &Total&37,931&12.62&59,507\\
    Marketing&Training ($t$)&15,346&12.55&24,304\\
    &Under-sampled ($u$)&3,806&50.60&24,304\\
    &Rejection-sampled ($r$)&1,644&52.43&20,621\\
    &Over-sampled ($o$)&22,625&40.69&207,978\\
    &Validation&11,354&12.30&16,154\\
    &Testing&11,231&13.04&19,048\\
    \hline
  \end{tabular}
  \caption{Summary of the datasets}
  \label{tab:6:databases}
\end{table}
 


\subsection{Results}


  For the experiments we first used three classification algorithms, decision tree ($DT$), logistic 
  regression ($LR$) and random forest ($RF$). Using the implementation of \textit{Scikit-learn} 
  \citep{Pedregosa2011}, each algorithm is trained using the different training sets: training 
  ($t$), under-sampling ($u$), cost-proportionate rejection-sampling  ($r$) \citep{Zadrozny2003}   
  and   cost-proportionate over-sampling ($o$) \citep{Elkan2001}. Afterwards,  we evaluate the 
  results of  the algorithms using $BMR$ \citep{CorreaBahnsen2014}. Then, the cost-sensitive 
  logistic  regression ($CSLR$) \citep{CorreaBahnsen2014b} and cost-sensitive decision tree 
  ($CSDT$) \citep{CorreaBahnsen2015} were also evaluated. Lastly, we calculate the 
  proposed ensembles of cost-sensitive decision trees algorithms. In particular, using each of the 
  random inducer methods, bagging ($CSB$), pasting ($CSP$), random forests ($CSRF$) and random 
  patches ($CSRP$), and then blending the base classifiers using each one of the combination 
  methods; majority voting ($mv$), cost-sensitive weighted voting ($wv$) and cost-sensitive 
  stacking ($s$). Unless otherwise stated, the random selection of the training set was repeated 50 
  times, and in each time the models were trained and results collected, this allows us to measure 
  the stability of the results. The implementation of the cost-sensitive algorithms is done using 
  the \textit{CostCla} library, see Appendix~\ref{app:1}.
  
    The results are shown in \tablename{ \ref{tab:9:results_savings}}. First, when observing  the 
  results of the cost-insensitive methods ($CI$), that is, $DT$, $LR$ and $RF$ algorithms trained 
  on the $t$ and $u$ sets, the $RF$ algorithm produces the best result by savings in three out of 
  the five sets, followed by the $LR-u$. It is also clear that the results on the $t$ dataset are 
  not as good as the ones on the $u$, this is highly related to the unbalanced distribution of the 
  positives and negatives in all the databases.

\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{l l r@{\hskip 0in}c@{\hskip 0in}l r@{\hskip 0in}c@{\hskip 0in}l r@{\hskip 
    0in}c@{\hskip 0in}l  } %sum 7.7
    \hline
    \bf{Family} & \bf{Algorithm} & \multicolumn{3}{c}{\bf{Fraud}} & 
    \multicolumn{3}{c}{\bf{Churn}} & \multicolumn{3}{c}{\bf{Credit 1}} \\ 
    \hline
CI&DT-t & 0.3176 &$\pm$& 0.0357 & -0.0018 &$\pm$& 0.0194 & 0.1931 &$\pm$& 0.0087 \\
&LR-t & 0.0092 &$\pm$& 0.0002 & -0.0001 &$\pm$& 0.0002 & 0.0177 &$\pm$& 0.0126 \\
&RF-t & 0.3342 &$\pm$& 0.0156 & -0.0026 &$\pm$& 0.0079 & 0.1471 &$\pm$& 0.0071\\
&DT-u & 0.5239 &$\pm$& 0.0118 & -0.0389 &$\pm$& 0.0583 & 0.3287 &$\pm$& 0.0125 \\
&LR-u & 0.1243 &$\pm$& 0.0387 & 0.0039 &$\pm$& 0.0492 & 0.4118 &$\pm$& 0.0313 \\
&RF-u & 0.5684 &$\pm$& 0.0097 & 0.0433 &$\pm$& 0.0533 & 0.4981 &$\pm$& 0.0079 \\
\hline 
CPS&DT-r & 0.3439 &$\pm$& 0.0453 & 0.0054 &$\pm$& 0.0568 & 0.3310 &$\pm$& 0.0126 \\
&LR-r & 0.3077 &$\pm$& 0.0301 & 0.0484 &$\pm$& 0.0375 & 0.3965 &$\pm$& 0.0263 \\
&RF-r & 0.3812 &$\pm$& 0.0264 & 0.1056 &$\pm$& 0.0412 & 0.4989 &$\pm$& 0.0080 \\
&DT-o & 0.3172 &$\pm$& 0.0274 & 0.0251 &$\pm$& 0.0195 & 0.1738 &$\pm$& 0.0092 \\
&LR-o & 0.2793 &$\pm$& 0.0185 & 0.0316 &$\pm$& 0.0228 & 0.3301 &$\pm$& 0.0109 \\
&RF-o & 0.3612 &$\pm$& 0.0295 & 0.0205 &$\pm$& 0.0156 & 0.2128 &$\pm$& 0.0081 \\
\hline 
BMR&DT-t-BMR & 0.6045 &$\pm$& 0.0386 & 0.0298 &$\pm$& 0.0145 & 0.1054 &$\pm$& 0.0358 \\
&LR-t-BMR & 0.4552 &$\pm$& 0.0203 & 0.1082 &$\pm$& 0.0316 & 0.2189 &$\pm$& 0.0541 \\
&RF-t-BMR & 0.6414 &$\pm$& 0.0154 & 0.0856 &$\pm$& 0.0354 & 0.4924 &$\pm$& 0.0087 \\
\hline 
CST&CSLR-t & 0.6113 &$\pm$& 0.0262 & 0.1118 &$\pm$& 0.0484 & 0.4554 &$\pm$& 0.1039 \\
&CSDT-t & 0.7116 &$\pm$& 0.2557 & 0.1115 &$\pm$& 0.0378 & 0.4829 &$\pm$& 0.0098 \\
\hline 
ECSDT&CSB-mv-t & 0.7124 &$\pm$& 0.0162 & 0.1237 &$\pm$& 0.0368 & 0.4862 &$\pm$& 0.0102 \\
&CSB-wv-t & 0.7276 &$\pm$& 0.0116 & 0.1539 &$\pm$& 0.0255 & 0.4862 &$\pm$& 0.0102 \\
&CSB-s-t & 0.7181 &$\pm$& 0.0109 & 0.1441 &$\pm$& 0.0364 & 0.4847 &$\pm$& 0.0096 \\
&CSP-mv-t & 0.7106 &$\pm$& 0.0113 & 0.1227 &$\pm$& 0.0399 & 0.4853 &$\pm$& 0.0104 \\
&CSP-wv-t & 0.7244 &$\pm$& 0.0202 & 0.1501 &$\pm$& 0.0302 & 0.4854 &$\pm$& 0.0105 \\
&CSP-s-t & 0.7212 &$\pm$& 0.0067 & 0.1488 &$\pm$& 0.0272 & 0.4848 &$\pm$& 0.0084 \\
&CSRF-mv-t & 0.6498 &$\pm$& 0.0598 & 0.0300 &$\pm$& 0.0488 & 0.4980 &$\pm$& 0.0120 \\
&CSRF-wv-t & 0.7249 &$\pm$& 0.0742 & 0.0624 &$\pm$& 0.0477 & 0.4979 &$\pm$& 0.0124 \\
&CSRF-s-t & 0.6731 &$\pm$& 0.0931 & 0.0586 &$\pm$& 0.0507 & 0.4839 &$\pm$& 0.0160 \\
&CSRP-mv-t & 0.7220 &$\pm$& 0.0082 & 0.1321 &$\pm$& 0.0280 & \bf{0.5154} &\bf{$\pm$}& \bf{0.0077} \\
&CSRP-wv-t & \bf{0.7348} &\bf{$\pm$}& \bf{0.0131} & 0.1615 &$\pm$& 0.0252 & 0.5152 &$\pm$& 0.0083 \\
&CSRP-s-t & 0.7336 &$\pm$& 0.0108 & \bf{0.1652} &\bf{$\pm$}& \bf{0.0264} & 0.4989 &$\pm$& 0.0088 \\
  \hline
  \multicolumn{11}{c}{(those models with the highest savings are market as bold)}
  \end{tabular}
    \caption{Results of the algorithms measured by savings}
    \label{tab:9:results_savings}
  \end{table}
\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{l l r@{\hskip 0in}c@{\hskip 0in}l r@{\hskip 0in}c@{\hskip 0in}l  } %sum 7.7
    \hline
    \bf{Family} & \bf{Algorithm} &  \multicolumn{3}{c}{\bf{Credit 2}} 
& \multicolumn{3}{c}{\bf{Marketing}} \\ 
    \hline
CI&DT-t & -0.0616 &$\pm$& 0.0229 & -0.2342 &$\pm$& 0.0609\\ 
&LR-t & 0.0039 &$\pm$& 0.0012 & -0.2931 &$\pm$& 0.0602\\ 
&RF-t & 0.0303 &$\pm$& 0.0040 & -0.2569 &$\pm$& 0.0637\\ 
&DT-u & -0.1893 &$\pm$& 0.0314 & -0.0278 &$\pm$& 0.0475\\ 
&LR-u &  0.1850 &$\pm$& 0.0231 & 0.2200 &$\pm$& 0.0376\\ 
&RF-u & 0.1237 &$\pm$& 0.0228 & 0.1227 &$\pm$& 0.0443\\ 
\hline 
CPS&DT-r & 0.0724 &$\pm$&0.0212 & 0.1960 &$\pm$& 0.0527\\ 
&LR-r &  0.2650 &$\pm$& 0.0115 & 0.4210 &$\pm$& 0.0267\\ 
&RF-r & 0.3055 &$\pm$& 0.0106 & 0.3840 &$\pm$& 0.0360\\ 
&DT-o &  0.0918 &$\pm$&0.0225 & -0.2598 &$\pm$& 0.0559\\ 
&LR-o &  0.2554 &$\pm$&0.0090 & 0.3129 &$\pm$& 0.0277\\ 
&RF-o &  0.2242 &$\pm$&0.0070 & -0.1782 &$\pm$& 0.0618\\ 
\hline 
BMR&DT-t-BMR & 0.2740 &$\pm$& 0.0067 & 0.4598 &$\pm$& 0.0089\\ 
&LR-t-BMR & \bf{0.3148} &\bf{$\pm$}& \bf{0.0094} & \bf{0.4973} &\bf{$\pm$}& \bf{0.0084}\\ 
&RF-t-BMR & 0.3133 &$\pm$& 0.0094 & 0.4807 &$\pm$& 0.0093\\ 
\hline 
CST&CSLR-t & 0.2748 &$\pm$&0.0069 & 0.4484 &$\pm$& 0.0072\\ 
&CSDT-t &  0.2835 &$\pm$& 0.0078 & 0.4741 &$\pm$& 0.0063\\ 
\hline 
ECSDT&CSB-mv-t &  0.2945 &$\pm$& 0.0105 & 0.4837 &$\pm$& 0.0078\\ 
&CSB-wv-t & 0.2948 &$\pm$&0.0106 & 0.4838 &$\pm$& 0.0079\\ 
&CSB-s-t &  0.2856 &$\pm$& 0.0088 & 0.4769 &$\pm$& 0.0078\\ 
&CSP-mv-t & 0.2919 &$\pm$& 0.0097 & 0.4831 &$\pm$& 0.0081\\ 
&CSP-wv-t & 0.2921 &$\pm$& 0.0098 & 0.4832 &$\pm$& 0.0082\\ 
&CSP-s-t &  0.2870 &$\pm$& 0.0084 & 0.4752 &$\pm$& 0.0089\\ 
&CSRF-mv-t &  0.2274 &$\pm$& 0.0520 & 0.3929 &$\pm$& 0.0655\\ 
&CSRF-wv-t &  0.2948 &$\pm$& 0.0079 & 0.4728 &$\pm$& 0.0125\\ 
&CSRF-s-t &  0.2518 &$\pm$& 0.0281 & 0.3854 &$\pm$& 0.0899\\ 
&CSRP-mv-t & 0.3053 &$\pm$& 0.0087 & 0.4960 &$\pm$& 0.0075\\ 
&CSRP-wv-t & 0.3015 &$\pm$& 0.0086 & 0.4885 &$\pm$& 0.0076\\ 
&CSRP-s-t & 0.2956 &$\pm$& 0.0078 & 0.4878 &$\pm$& 0.0080\\ 
  \hline
  \multicolumn{8}{c}{(those models with the highest savings are market as bold)}
  \end{tabular}
    \caption{Continuation of \tablename{~\ref{tab:9:results_savings}}}
    \label{tab:9:results_savings2}
  \end{table}
  










