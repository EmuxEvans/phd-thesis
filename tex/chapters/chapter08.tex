\chapter{Ensembles of cost-sensitive decision trees}

\begin{remark}{Outline}
In this chapter, we introduce the framework of ensembles of example-dependent cost-sensitive 
decision-trees, by training example-dependent cost-sensitive decision trees using four different 
random inducer methods and then blending them using three different combination approaches.
First, in Section \ref{sec:8:ensemble}, we give the background behind ensemble learning. Then, in 
Section \ref{sec:8:ecsdt}, we present our previously proposed ensembles of cost-sensitive 
decision-trees framework. Moreover, in Section \ref{sec:8:theoretical}, we prove theoretically that 
combining individual cost-sensitive classifiers achives better  results in the sense of higher 
financial savings. Afterwards, in Section \ref{sec:8:experiments}, we compare the results of the 
proposed algorithm, against state-of-the-art methods, using the five real-world cost-sensitive 
databases. Finally, in Section \ref{sec:8:experiments_all}, we compare the results of the different 
algorithms presented in this Thesis, using the different databases.
\end{remark}


\section{Ensemble methods}
\label{sec:8:ensemble}

  Ensemble learning is a widely studied topic in the machine learning community. The main
  idea behind the ensemble methodology is to combine several individual base classifiers in
  order to have a classifier that outperforms each of them \citep{Rokach2009}. Nowadays, 
  ensemble methods are  one of the most popular and well studied machine learning techniques 
  \citep{Zhou2012}, and it can be noted that since 2009 all the first-place and 
  second-place winners of the KDD-Cup competition\footnote{https\://www.sigkdd.org/kddcup/} used 
  ensemble methods. The core principle in ensemble learning, is to induce random perturbations into 
  the learning procedure in order to produce several different base classifiers from a single 
  training set, then combining the base classifiers in order to make the final prediction.
  In order to induce the random permutations and therefore create the different base classifiers, 
  several methods have been proposed, in particular: bagging \citep{Breiman1996}, 
  pasting~\citep{Breiman1999}, random forests \citep{Breiman2001} and random patches 
  \citep{Louppe2012}. Finally, after  the base   classifiers are trained, they are typically 
  combined using either   majority voting,  weighted  voting    or  stacking~\citep{Zhou2012}.

As shown in \figurename{~\ref{fig:8:1}, there are three main reasons regarding why ensemble 
methods perform better than single models: statistical, computational and representational 
\citep{Dietterich2000a}. First, from a statistical point of view, when the learning set is too 
small, an algorithm can find several good models withing the search space, that arise to the same 
performance on the training set $\mathcal{S}$. Nevertheless, without a validation set, there is 
the risk of choosing the wrong model. The second reason is computational; in general, algorithms 
rely on some local search optimization and may get stuck in a local optima. Then, an ensemble may 
solve this by focusing different algorithms to different spaces across the training set. The last 
reason is representational. In most cases, for a learning set of finite size, the  true function 
$f$ cannot be represented by any of the candidate models. By combining several  models in an 
ensemble, it may be possible to obtain a model with a larger coverage across the  space of 
representable functions.
  
\begin{figure}[t!]
\includegraphics[scale=.5]{ch8_fig1}
\caption{Main reasons regarding why ensemble methods perform better than 
  single models: statistical, computational and representational \citep{Dietterich2000a}.}
\label{fig:8:1}
\end{figure} 
  
  The most typical form of an ensemble is made by combining $T$ different base classifiers.
  Each  base classifier $M(\mathcal{S}_j)$ is trained by applying algorithm $M$ to a random subset 
  $\mathcal{S}_j$ of the training set $\mathcal{S}$.  %  $\mathcal{S_j} = RI(\mathcal{S})$
  For simplicity we define $M_j \equiv  M(\mathcal{S}_j)$ for $j=1,\dots,T$, and 
  $\mathcal{M}=\{M_j\}_{j=1}^{T}$ a set of base classifiers.
  Then, these models are combined using majority voting to create the ensemble $H$ as follows
  \begin{align}\label{eq:8:majority-vote}
    f_{mv}(\mathcal{S},\mathcal{M}) = \argmax_{c \in \{0,1\}} \sum_{j=1}^T 
    \mathbf{1}_c(M_j(\mathcal{S})).
  \end{align}

  \newpage
  \begin{remark}{Theoretical performance of an ensemble}
%   Moreover, i
  If we assume that each one of the $T$ base classifiers have a probability $\rho$ of 
  being correct, the probability of an ensemble making the correct decision, denoted by $P_c$,
  can be calculated using the binomial \mbox{distribution \citep{Hansen1990}}
  \begin{equation}\label{eq:8:prob}
    P_c = \sum_{j>T/2}^{T} {{T}\choose{j}} \rho^j(1-\rho)^{T-j}.
  \end{equation}
  Furthermore, as shown in \citep{Lam1997}, if $T\ge3$ then:
  \begin{itemize}
    \item If $\rho>0.5$, then $ \lim_{T \to  \infty} P_c=1 $
    \item If $\rho<0.5$, then $ \lim_{T \to  \infty} P_c=0 $
    \item If $\rho=0.5$, then $P_c=0.5$ for any $T$.
  \end{itemize}
  Leading to the conclusion that $P_c\ge \rho$ if $\rho \ge 0.5$ for any $T$.
  \end{remark}
  
\subsection{Cost-sensitive ensembles}

  In the context of cost-sensitive classification, some authors have proposed methods for using 
  ensemble techniques. In \citep{Masnadi-shirazi2011}, the authors proposed a framework for 
  cost-sensitive boosting that is expected to minimized the losses by using optimal cost-sensitive 
  decision rules. In \citep{Street2008}, a bagging algorithm with adaptive costs was proposed. In 
  his doctoral thesis, Nesbitt \citep{Nesbitt2010}, proposed a method for cost-sensitive 
  tree-stacking. In this method different decision trees are learned, and then combined in a way 
  that a cost function is minimized. Lastly in \citep{Lomax2013}, a survey of application of 
  cost-sensitive learning with decision trees is shown, in particular including other methods that 
  create cost-sensitive ensembles. However, in all the previously presented method, the 
  misclassification costs only dependent on the class, therefore, assuming a constant cost across 
  examples. Given that, these methods are not  well suited for example-dependent cost-sensitive 
  problems. 

      
\section{Ensembles of cost-sensitive decision trees}
\label{sec:8:ecsdt}

In this section we shown our previously proposed framework of ensembles of  example-dependent 
cost-sensitive  decision-trees \citep{CorreaBahnsen2015b}, by training example-dependent 
cost-sensitive decision trees using four different  random inducer methods and then blending them 
using three different combination approaches. Moreover, we propose two new cost-sensitive 
combination approaches, cost-sensitive weighted  voting and cost-sensitive stacking. The latter 
being an extension of our previously proposed cost-sensitive logistic regression 
\citep{CorreaBahnsen2014b}. 

The remainder of the section is organized as follows: First, we introduce the example-dependent 
cost-sensitive decision tree. Then we present the different random inducers and combination 
methods. Finally, we define our proposed algorithms.
\todo{fix intro}


\subsection{Random inducers}

With the objective of creating an ensemble of example-dependent cost-sensitive decision trees, we 
first create $T$ different random subsamples $\mathcal{S}_j$ for $j=1,\dots,T$, of the training  set 
$\mathcal{S}$, and train a $CSDT$ algorithm on each one. In particular we create the different 
subsets using four different methods: bagging \citep{Breiman1996}, pasting \citep{Breiman1999}, 
random forests \citep{Breiman2001} and random patches \citep{Louppe2012}. 

In bagging \citep{Breiman1996}, base classifiers are built on randomly drawn bootstrap subsets of 
the original data, hence producing different base classifiers. Similarly, in pasting 
\citep{Breiman1999}, the base classifiers are built on random  samples without replacement from 
the training set. In random forests \citep{Breiman2001}, using decision trees as the base learner, 
bagging   is extended and   combined  with a  randomization of the input features that  are used 
when  considering candidates  to split    internal nodes. In particular, instead of looking for  
the best  split among all   features, the   algorithm selects, at each node, a random subset of 
features  and then determines   the best split only over  these features. In the random patches   
algorithm \citep{Louppe2012}, base classifiers are created by randomly     drawn bootstrap subsets 
of both examples and features. To further clarify the difference between the random inducer 
methods, in \figurename{~\ref{fig:8:2}}, we show a visual representation of the random inducers 
algorithms.
 
\begin{figure}[t!]
\includegraphics[scale=.5]{ch8_fig2}
\caption{Visual representation of the random inducers algorithms.}
\label{fig:8:2}
\end{figure} 


\subsection{Combination methods}
  Lastly, the base classifiers are combined using either majority voting, cost-sensitive weighted 
  voting and cost-sensitive stacking. Majority voting consists in collecting the predictions of 
  each base classifier and selecting the decision with the highest number of votes, see 
  (\ref{eq:8:majority-vote}).

  \subsubsection*{Cost-sensitive weighted voting}

  This method is an extension of weighted voting. First, in the traditional approach, a 
  similar comparison of the votes of the base classifiers is made, but giving a weight $\alpha_j$ 
  to each classifier $j$ during the voting phase \citep{Zhou2012}
  \begin{align} \label{eq:8:weighted-majority-vote}
    f_{wv}(\mathcal{S},\mathcal{M}, \alpha)
    =\argmax_{c \in \{0,1\}} \sum_{j=1}^T \alpha_j \mathbf{1}_c(M_j(\mathcal{S})),
  \end{align}
  where $\alpha=\{\alpha_j\}_{j=1}^T$.
  The calculation of $\alpha_j$ is related to the performance of each classifier $M_j$.
  It is usually defined as the normalized misclassification error   $\epsilon$ of the base 
  classifier $M_j$  in the out of bag set   $\mathcal{S}_j^{oob}=\mathcal{S}-\mathcal{S}_j$
  \begin{equation}
    \alpha_j=\frac{1-\epsilon(M_j(\mathcal{S}_j^{oob}))}{\sum_{j_1=1}^T 
    1-\epsilon(M_{j_1}(\mathcal{S}_{j_1}^{oob}))}.
  \end{equation}

  However, as discussed in Section \ref{sec:edcs}, the misclassification measure is not suitable in 
  many real-world classification problems. We propose a method to calculate the weights $\alpha_j$ 
  taking into account the actual savings of the classifiers. Therefore using (\ref{eq:savings}), we 
  define
  \begin{equation}
    \alpha_j=\frac{Savings(M_j(\mathcal{S}_j^{oob}))}
    {\sum_{j_1=1}^T Savings(M_{j_1}(\mathcal{S}_j^{oob}))}.
  \end{equation}
  This method guaranties that the base classifiers that contribute to a higher increase in savings 
  have more importance in the ensemble.
  
  \subsubsection*{Cost-sensitive stacking}
  
  The staking method consists in combining the different base classifiers by learning a 
  second level algorithm on top of them \citep{Wolpert1992}. In this framework, once the base 
  classifiers are constructed using the training set  $\mathcal{S}$, a new set is constructed 
  where the output of the base classifiers  are now considered as the features while keeping the 
  class labels.
  
  Even though there is no restriction on which algorithm can be used as a second level learner, 
  it is common to use a lineal model \citep{Zhou2012}, such as 
  \begin{align}
    f_s(\mathcal{S},\mathcal{M},\mathcal{\beta}) =
    g \left( \sum_{j=1}^T \beta_j M_j(\mathcal{S}) \right),
  \end{align}
  where $\mathcal{\beta}=\{\beta_j\}_{j=1}^T$, and $g(z)$ is the sign function 
  \mbox{$g(z)=sign(z)$} in the case of a lineal regression or the sigmoid function, defined 
  as \mbox{$g(z)=1/(1+e^{-z})$}, in the case of a logistic regression. 
  
  Moreover, following the logic used in \citep{Nesbitt2010}, we propose learning the set of  
  parameters $\mathcal{\beta}$  using our proposed cost-sensitive logistic regression ($CSLR$) 
  \citep{CorreaBahnsen2014b}. The $CSLR$ algorithm consists in introducing example-dependent costs 
  into a logistic regression, by changing the objective function of the model to one that is 
  cost-sensitive. For the specific case of cost-sensitive stacking, we define the cost function as: 
  \begin{align}
    &J(\mathcal{S},\mathcal{M},\beta)= \nonumber \\
    & \sum_{i=1}^{N} \bigg[ y_i\bigg( 
    f_s(\mathbf{x}_i,\mathcal{M},\mathcal{\beta}) \cdot(C_{TP_i} - C_{FN_i}) + C_{FN_i} \bigg) + 
    \nonumber \\
    & (1-y_i)\bigg( f_s(\mathbf{x}_i,\mathcal{M},\mathcal{\beta}) \cdot(C_{FP_i} - C_{TN_i}) 
      +C_{TN_i} \bigg) \bigg].
  \end{align}
  Then, the parameters $\beta$ that minimize the logistic cost function are used in order to 
  combine the different base classifiers. However, as discussed in \citep{CorreaBahnsen2014b}, 
  this cost function is not convex for all possible cost matrices, therefore, we use genetic 
  algorithms to minimize it.
  
  Similarly to cost-sensitive weighting, this method guaranties that the base classifiers that 
  contribute to a higher increase in savings have more importance in the ensemble. Furthermore, 
  by learning an additional second level cost-sensitive method, the combination is made such that 
  the overall   savings measure is maximized.


\section{Theoretical analysis of the ECSDT}
\label{sec:8:theoretical}

    
\section{Experiments}
\label{sec:8:experiments}

\section{Overall experiments}
\label{sec:8:experiments_all}