\chapter{Cost-sensitive decision trees}

\begin{remark}{Outline}
In this chapter, we present the well-known family of \textit{random forests}
methods. In Section~\ref{sec:4:bias-variance}, we first describe the bias-variance
decomposition of the prediction error and then present, in
Section~\ref{sec:4:ensemble}, how aggregating randomized models through
ensembles reduces the prediction error by decreasing the variance term in this
decomposition. In Section~\ref{sec:4:random-forests}, we revisit random forests
and its variants and study how randomness introduced into the decision trees
reduces prediction errors by decorrelating the decision
trees in the ensemble. Properties and features of random forests are then outlined
in Section~\ref{sec:4:features} while their consistency
is finally explored in Section~\ref{sec:4:consistency}.
\end{remark}

\section{Decision trees}
Decision trees are one of the most widely used machine learning algorithms \citep{Lior2008}. 
The technique is considered to be white box, in the sense that is easy to interpret, and has a 
very low computational cost, while maintaining a good performance as compared with more complex 
techniques \citep{Hastie2009}. There are two types of decision tree depending on the objective of 
the model. They work either for classification or regression. In this section we focus on
binary classification decision tree.

\subsection{Splitting criteria}
 
 In decision trees the splitting criteria is a measure that evaluates how a splitting threshold of 
a particular feature
 $(x^j,l^j_m)$ performs on a given set $S$.
\begin{equation}
 S = X_i\mbox{ for }  i=1,2,...,N_s
\end{equation}
 In order to evaluate the splitting criteria first the training data $S$ must be split in $S^l$ and 
$S^r$ 
 according to the splitting rule $(x^j,l^j_m)$:
 \begin{equation}
 S^l = \{X \vert X_i \in S \wedge x^j_i \le l^j_m \}
 \end{equation}
 \begin{equation}
 S^r = \{X \vert X_i \in S \wedge x^j_i > l^j_m \}
 \end{equation}
  where
 \begin{equation}
 S = S^l \cup S^r
 \end{equation}
 Afterwards, the percentage of positives $p$ is calculated for $S$ and each of the subsets:
  \begin{equation}
 p_S= P(S(y)= 1\vert S)=\frac{\vert (S\vert S(y)=1) \vert}{\vert S \vert}
 \end{equation}
 \begin{equation}
 p^l_S= P(S(y)= 1\vert S^l)=\frac{\vert (S^l\vert S^l(y)=1) \vert}{\vert S^l \vert}
 \end{equation}
 \begin{equation}
 p^r_S= P(S(y)= 1\vert S^r)=\frac{\vert (S^r\vert S^r(y)=1) \vert}{\vert S^r \vert}
 \end{equation} 
  Then, the impurity of each leaf is calculated using either a misclassification error, entropy or 
Gini measures:
 \vspace{10pt}

 Misclassification:
 \begin{equation}
 I_m(p)=1-max(p,1-p)
 \end{equation}

 Entropy:
 \begin{equation}
 I_e(p)=-p\log p -(1-p) \log (1-p)
 \end{equation}
 
 Gini:
 \begin{equation}
 I_g(p)=2p(1-p)
 \end{equation}
 In \figurename{ \ref{measures1}}, the three measures are shown. All measures are similar, with the 
 exception that the gini index and the cross-entropy are differentiable, which makes them
 more suitable for numerical optimization.
 \vspace{10pt}

  \begin{figure}[t!]
 \includegraphics[scale=0.85]{ch8_fig1}
 \caption{Impurity measures for a binary classification, as a function of the proportion of
 positive examples in the set. *Cross-entropy is scaled.\citep{Hastie2009} }
 \label{measures1}
 \end{figure} 

 Finally the splitting criteria gain of using the splitting rule $(x^j,l^j_m)$ is calculated as 
 the impurity of $S$ minus the weighed impurity of each leaf:
 \begin{equation}
 Gain((x^j,l^j_m),S)=I(p_S)-\frac{\vert S^l \vert}{\vert S \vert}I(p^l_S)
 -\frac{\vert S^r \vert}{\vert S \vert}I(p^r_S)
 \end{equation} 
 where $I(p)$ can be either of the impurity measures $I_m(p)$, $I_e(p)$ or $I_g(p)$.
 
 \subsection{Tree growing}
 In order to grow a tree typical algorithms use a top-down induction using a greedy search in each
 iteration \citep{Rokach2010}. In each iteration, the algorithms evaluates all possible splitting 
rules
 and pick the one that maximizes the splitting criteria. After the selection of a splitting rule,
 each leaf is further selected and it is subdivides into smaller leafs, until one of the stopping 
criteria is meet.
 In \figurename{ \ref{program1}, the pseudocode of the tree growing procedure is presented.

\todo{figure with steps to construct a decision tree}
  
   \subsection{Stopping criteria}
 As the growing phase of the algorithm continue, at each iteration the stopping criteria are 
evaluated \citep{Rokach2010}.
 The most common stopping criteria are:
 \begin{enumerate}
  \item All examples in the set belong to the same class.
  Meaning that either $p_S=1$ or $p_S=0$.

  \item The maximum number of iterations have been reached.
  
  \item The number of examples in $S$ are less than the minimum number of examples defined for a 
split.
  
  \item The number of examples in $S^l$ or $S^r$ are less than the minimum number of examples 
defined for a leaf.
  
  \item The best splitting rule has a Gain lower than a defined threshold.
  
 \end{enumerate}
 
\subsection{Pruning techniques}
 After a decision tree has been fully grown, there is the big chance that 
 the algorithm generates a large tree that is probably over fitting the
 training data. In order to solve this in \citep{Breiman1984a} originally suggest
 the use of pruning techniques after the tree growing phase.
 The overall objective of pruning is to eliminate branches that are not contributing
 to the generalization accuracy of the \mbox{tree \citep{Rokach2010}.}
 
 In general pruning techniques start from a fully grown tree, and recursively check if
 by eliminating a $Branch$ there is an $Improvement$ in the error rate $\epsilon$ of the $Tree$.
  There are two main methods to calculate the $Improvement$ of the error rate $\epsilon$ of the 
$Tree$,
  cost-complexity and  error based pruning:
  
  \begin{itemize}
   \item Cost-complexity pruning:
   
      Initially proposed by Breiman  \citep{Breiman1984a}, this method evaluate iterative if
      the removal of a $Branch$ improve the error rate $\epsilon$ of a $Tree$, weighted by 
      the difference of the number of leafs trees.
      \begin{equation}
      PC_{cc} = \frac{\epsilon(EB(Tree,branch),S)-\epsilon(Tree,S) }
      {\vert Tree\vert-\vert EB(Tree,branch)\vert}
      \end{equation} 
      where $\epsilon(Tree,S)$ calculate the error rate of the $Tree$ in
      set $S$ and $EB(Tree,branch)$ is an auxiliary function that 
      removes $branch$ from $Tree$ and return a new $Tree$.
      
      At each iteration, the current $Tree$ is compared against all possible $Branches$.
      

   \item Error based pruning:
   
      This method proposed by Quinlan  \citep{Quinlan1992} is more pessimistic and instead of
      using the error rate $\epsilon$ calculate the upper bound of the confidence interval
      of the error $\epsilon_{UB}$ calculated assuming a normal distribution with a defined 
significance level $Z_\alpha$.
      \begin{equation}
      PC_{eb} = \frac{\epsilon_{UB}(EB(Tree,branch),S)-\epsilon_{UB}(Tree,S) }
      {\vert Tree\vert-\vert EB(Tree,branch)\vert}
      \end{equation} 
      where 
      \begin{equation}
      \epsilon_{UB}(Tree,S)= \epsilon(Tree,S)+Z_\alpha \left( 
      \frac{\epsilon(Tree,S)(1-\epsilon(Tree,S))}{\vert S \vert} 
      \right)^{(\frac{1}{2})}
      \end{equation}   
      Moreover, as a difference from cost-complexity pruning, in the error based method, 
      at each iteration not all the branches are compared, but only the ones on the same level.
      Meaning leafs are only compared against other leafs that are in the same level of iterations
      when the $Tree$ was constructed.
      This allows for much faster pruning procedure.
  \end{itemize}
  
  \todo{figure with the steps for pruning}
  
  \todo{ compare with paper DT}
  
   In \figurename{ \ref{program2}, the general method of pruning is shown. The pruning procedure 
consists in
  evaluating the $Improvement$ of eliminating all possible branches in a $Tree$, and then eliminate 
the one
  with the higher $Improvement$ and repeat the process until a stopping criteria is meet.

 
  \subsection{Categorical features}
  When using continuous features the splitting is done by testing all possible thresholds on the 
database and
  selecting the one that maximizes the desired measure. Nevertheless, this is not straight forward 
when using a
  categorical feature, since testing all possible ways of binning the feature becomes prohibitively 
time 
  consuming the feature has many categories. Instead, a common method for doing this is to calculate
  $p$ of each category, then sort it, and applied the method as a continuous feature 
\citep{Marslan}.
 
  \section{Decision tree algorithms}
  Two main branches of decision trees has being studied in the last years. The main difference is 
the
  impurity measure used when splitting. First the CART algorithm that is based in the Gini index, 
and
  later the ID3 and C4.5 which uses the entropy measure.
  
  \subsection{CART}
  CART or classification and regression trees was introduced by Brieman et al. in 1984 
\citep{Breiman1984a}.
  It is based on using the Gini index as the impurity measure and the tree is grow until all 
examples in
  each leaf belong to the same class. Afterwards, the tree is pruned using the cost-complexity
  method \citep{Rokach2010,Marslan}.
  
  \subsection{ID3}
  The ID3 algorithm uses entropy as the impurity measure. The growing of the tree stop when all 
examples belong 
  of each leaf belongs to the same class. In ID3 no pruning is applied \citep{Quinlan1992}
  \subsection{C4.5}
  C4.5 the extension of ID3 both proposed by Quinlan \citep{Quinlan1992}.
  Both are similar regarding the measure used, but C4.5 define the stopping criteria during the 
growth process
  to be when the number of examples in a set is less than a threshold. Moreover, after the tree is 
created
  a error based pruning is applied \citep{Rokach2010}.
 


\section{Example-Dependent Cost-sensitive Decision Trees}

Standard decision tree algorithms focus on inducing trees that maximize accuracy. However this is 
	not optimal when the misclassification costs are unequal \citep{Elkan2001}.This has led to many 
	studies that develop algorithms that aim to introduce the cost-sensitivity into the algorithms 
	\citep{Lomax2013}. These studies have focused on introducing the class-dependent costs  
	\citep{Draper1994,Ting2002,Ling2004,Li2005,Kretowski2006,Vadera2010}, which is not optimal for 
	some applications. For example in credit card fraud detection, it is true that false positives 
	have a different cost than false negatives, nevertheless, false negatives may vary significantly, 
	which makes class-dependent cost-sensitive methods not suitable for this problem.
      
  In this section, we first propose a new method to introduce the costs into the decision tree 
	induction stage, by creating new-cost based impurity measures. Afterwards, we propose a new 
	pruning method based on minimizing the cost as pruning criteria.

	\subsection{Cost-sensitive impurity measures}

		Standard impurity measures such as misclassification, entropy or Gini, take into account the 
		distribution of classes of each leaf to evaluate the predictive power of a splitting rule,
		leading to an impurity measure that is based on minimizing the misclassification rate. However, 
		as has been previously shown \citep{CorreaBahnsen2013}, minimizing misclassification does not 
		lead to the same results than minimizing cost. Instead, we are interested in measuring how good 
		is a splitting rule in terms of cost not only accuracy. For doing that, we propose a new 
		example-dependent cost based impurity measure that takes into account the cost matrix of each 
		example.

		We define a new cost-based impurity measure taking into account the costs when all the examples
		in a leaf are classified both as negative using $f_0$ and positive using $f_1$
		\begin{equation}\label{eq:cost_impurity}
			I_c(S) = \min \bigg\{ Cost(f_0(S)), Cost(f_1(S)) \bigg\}.
		\end{equation}
		The objective of this measure is to evaluate the lowest expected cost of a splitting rule.
		Following the same logic, the classification of each set is calculated as the prediction that 
		leads to the lowest cost
	  \begin{equation}\label{eq_pred}
	    f(S) = 
	    \begin{cases}
	      \phantom{-}0 \phantom{-} \mbox{if} \phantom{-} Cost(f_0(S)) \le Cost(f_1(S))\\
	      \phantom{-}1 \phantom{-}\mbox{otherwise}
	    \end{cases}
	  \end{equation}

		Finally, using the cost-based impurity, the splitting criteria cost based gain of using the 
		splitting rule $(X^j,l^j)$ is calculated with (\ref{eq:gain}). 

	\subsection{Cost-sensitive pruning}
 
		Most of the literature in class-dependent cost-sensitive decision tree focuses on using the 
		misclassification costs during the construction of the algorithms \citep{Lomax2013}. Only few 
		algorithms such as AUCSplit \citep{Ferri2002} have \mbox{included} the costs both during and 
		after the construction of the tree. However, this approach only used the class-dependent costs, 
		and not the example-dependent costs.
 
		We propose a new example-dependent cost-based impurity measure, by replacing the error rate 
		$\epsilon$ in (\ref{eq:pruning}) with the cost of using the $Tree$ on $S$ i.e. by replacing 
with 
		$Cost(f(S))$.
		\begin{equation}\label{eq:cost_pruning}
			PC_{c} = \frac{ Cost(f(S)) - Cost(f^*(S)) }
		  {\vert Tree\vert-\vert EB(Tree,node)\vert} ,
		\end{equation}
		where $f^*$ is the classifier of the tree without the selected node $EB(Tree,node)$.
 
		Using the new pruning criteria, nodes of the tree that do not contribute to the minimization of 
		the cost will be pruned, regardless of the impact of those nodes on the accuracy of the
		algorithm. This follows the same logic as in the proposed cost-based impurity measure, since 
		minimizing the misclassification is different than minimizing the cost, and in several 
		real-world applications the objectives align with the cost not with the misclassification error.
		
\section{Experiments}
