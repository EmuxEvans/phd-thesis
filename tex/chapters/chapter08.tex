\chapter{Cost-sensitive decision trees}\label{ch:8}

\begin{remark}{Outline}
In this chapter, we propose the cost-sensitive decision tree algorithm. The algorithm is 
based on incorporating the different example-dependent costs into a new cost-based impurity measure 
and a new cost-based pruning criteria. First, in Section~\ref{sec:8:dt}, we give the background 
behind the decision tree algorithm. Then, in Section~\ref{sec:8:csdt}, we introduce the 
cost-sensitive decision tree method. For this, we first show the new cost-sensitive impurity 
measure that introduces the different costs when evaluating the performance of a split. Then, we 
introduce a new cost-sensitive pruning criteria. Finally, in Section~\ref{sec:8:experiments}, we 
compare the results of the proposed algorithm against state-of-the-art methods using the five 
real-world cost-sensitive databases.
\end{remark}


\section{Introduction}
\label{sec:8:intro}

Decision trees are one of the most widely used machine learning algorithms \citep{Lior2008}. 
The technique is considered to be white box, in the sense that is easy to interpret, and has a 
very low computational cost, while maintaining a good performance as compared with more complex 
techniques \citep{Hastie2009}. There are two types of decision tree depending on the objective of 
the model. They work either for classification or regression. In this section we focus on
binary classification decision tree.

Standard decision tree algorithms focus on inducing trees that maximize accuracy. However this is 
not optimal when the misclassification costs are unequal \citep{Elkan2001}. This has led to many 
studies that develop algorithms that aim to introduce the cost-sensitivity into the algorithms 
\citep{Lomax2013}. These studies have focused on introducing the class-dependent costs  
\citep{Draper1994,Ting2002,Ling2004,Li2005,Kretowski2006,Vadera2010}, which is not optimal for 
some applications. For example in credit card fraud detection, it is true that false positives 
have a different cost than false negatives, nevertheless, false negatives may vary significantly, 
which makes class-dependent cost-sensitive methods not suitable for this problem.

In this chapter, we propose the cost-sensitive decision tree algorithm. The algorithm is 
based on incorporating the different example-dependent costs into a new cost-based impurity measure 
and a new cost-based pruning criteria. Then, we evaluate the proposed example-dependent 
cost-sensitive decision tree using three different databases. In particular, a credit card fraud 
detection, a credit scoring and a direct marketing databases. The results show that the proposed 
method outperforms state-of-the-art example-dependent cost-sensitive methods. Furthermore, when 
compared against a standard decision tree, our method builds significantly smaller trees in only a 
fifth of the time.

\section{Decision trees}
\label{sec:8:dt}

There are two types of decision tree depending on the objective of the model. They work either for 
classification or regression. In this section we focus on binary classification decision tree.

\subsection{Construction of classification trees}

Classification trees is one of the most common types of decision tree, in which the objective 
is to find the $Tree$ that best discriminates between classes. In general the decision tree 
represents a set of splitting rules organized in levels in a flowchart structure.

\subsubsection{Splitting criteria}

In the $Tree$, each rule is shown as a node, and it is represented as $(\mathbf{x}^j,l^j)$, meaning 
that the 
set $\mathcal{S}$ is split in two sets $\mathcal{S}^l$ and $\mathcal{S}^r$ according to 
$\mathbf{x}^j$ and $l^j$:
\begin{equation}
  \mathcal{S}^l = \{\mathbf{x}_i^* \vert \mathbf{x}_i^* \in \mathcal{S} \wedge x^j_i \le l^j \},
\end{equation}
and
\begin{equation}
  \mathcal{S}^r = \{\mathbf{x}_i^* \vert \mathbf{x}_i^* \in \mathcal{S} \wedge x^j_i > l^j \},
\end{equation}
where $\mathbf{x}^j$ is the $j^{th}$ feature represented in the vector 
$\mathbf{x}^j=[x_1^j,x_2^j,...,x_N^j]$, and $l^j$ is a value such that $min(\mathbf{x}^j) \le l^j < 
max(\mathbf{x}^j)$. Moreover, $\mathcal{S} = \mathcal{S}^l \cup \mathcal{S}^r$.

After the training set have been split, the percentage of positives in the different sets is 
calculated. First, the number of positives in each set is estimated by
\begin{equation}
 \mathcal{S}_1 = \{\mathbf{x}_i^* \vert \mathbf{x}_i^* \in \mathcal{S} \wedge y_i =1 \},
\end{equation}
then the percentage of positives is calculates as
\begin{equation}
 \pi_1=\vert \mathcal{S}_1 \vert / \vert \mathcal{S} \vert.
\end{equation}

Then, the impurity of each leaf is calculated using either a misclassification error, 
entropy or Gini measures:
\begin{itemize}
 \item  Misclassification
 \begin{equation}
 I_m(\pi_1)=1-max(\pi_1,1-\pi_1)
 \end{equation}
 
 \item Entropy
 \begin{equation}
 I_e(\pi_1)=-\pi_1\log \pi_1 -(1-\pi_1) \log (1-\pi_1)
 \end{equation}
 
 \item Gini
 \begin{equation}
 I_g(\pi_1)=2\pi_1(1-\pi_1)
 \end{equation}
\end{itemize}

In \figurename{ \ref{fig:8:impurity}}, the three measures are shown. All measures are similar, with 
the exception that the gini index and the cross-entropy are differentiable, which makes them
more suitable for numerical optimization.
\begin{figure}[t!]
\includegraphics[scale=0.85]{ch8_fig1}
\caption{Impurity measures for a binary classification, as a function of the proportion of
positive examples in the set (Cross-entropy is scaled) \citep{Hastie2009}.}
\label{fig:8:impurity}
\end{figure} 

Finally the gain of the splitting criteria using the rule $(\mathbf{x}^j,l^j)$ is calculated as the 
impurity of $\mathcal{S}$ minus the weighted impurity of each leaf:
\begin{equation}\label{eq:8:gain}
  Gain(\mathbf{x}^j,l^j)=I(\pi_1)-\frac{\vert \mathcal{S}^l \vert}{\vert \mathcal{S} 
\vert}I(\pi^l_1)  -\frac{\vert \mathcal{S}^r \vert}{\vert \mathcal{S} \vert}I(\pi^r_1),
\end{equation} 
where $I(\pi_1)$ can be either of the impurity measures $I_e(\pi_1)$ or $I_g(\pi_1)$.

Subsequently, the gain of all possible splitting rules is calculated. The rule with maximal 
gain is selected
\begin{equation}
  (best_x, best_l) = \argmax_{(\mathbf{x}^j,l^j)}Gain(\mathbf{x}^j,l^j),
\end{equation}
and the set $\mathcal{S}$ is split into $\mathcal{S}^l$ and $\mathcal{S}^r$ according to that rule. 


\subsubsection{Tree growing}
In order to grow a tree typical algorithms use a top-down induction using a greedy search in each
iteration \citep{Rokach2010}. In each iteration, the algorithms evaluates all possible splitting 
rules and pick the one that maximizes the splitting criteria. After the selection of a splitting 
rule, each leaf is further selected and it is subdivides into smaller leafs, until one of the 
stopping criteria is meet. In Algorithm \ref{alg:8:tree_grow}, the pseudocode of the tree 
growing procedure is presented.

\begin{algorithm}\label{alg:8:tree_grow}
Pseudocode of the tree growing procedure.
\textnormal{
\begin{algorithmic}[1]
\Function{TreeGrow}{$\mathcal{S}$}
    \State $Tree = Empty$
    \If{\Call{Stopping}{$Tree, \mathcal{S}$}}
      \State \Return $Tree$
     \EndIf
    \State \# For all features
    \For{$j=1 \  to \ k$}
      \State \# For all possible thresholds
      \For{$m=1 \ to N_k$}
        \State $Gains(j,m) = $ \Call{Gain}{$\mathbf{x}^j,l^j_m, \mathcal{S}$}
      \EndFor
    \EndFor
    \State \# Select the threshold with the highest gain
    \State $(j^*, l^*) = argmax_{(j,m)}Gains$
    \State \# Split the set using the best rule
    \State $\mathcal{S}^l = \{\mathbf{x}_i^* \vert \mathbf{x}_i^* \in \mathcal{S} 
            \wedge \mathbf{x}_i^{j^*} \le l^* \}$
    \State $\mathcal{S}^r = \{\mathbf{x}_i^* \vert \mathbf{x}_i^* \in \mathcal{S} 
            \wedge \mathbf{x}_i^{j^*} > l^* \}$
    \State \# Recursively continue growing tree
    \State \Call{TreeGrow}{$\mathcal{S}^l$}
    \State \Call{TreeGrow}{$\mathcal{S}^r$}
    \State \# Return the tree
    \State \Return $Tree$
\EndFunction
\end{algorithmic}
}
\end{algorithm}

  
\subsubsection{Stopping criteria}
As the growing phase of the algorithm continue, at each iteration the stopping criteria are 
evaluated \citep{Rokach2010}. The most common stopping criteria are:
 \begin{itemize}
   \item All examples in the set belong to the same class. Meaning that either $\pi_1=1$ or 
    $\pi_1=0$.

  \item The maximum number of iterations have been reached.
  
  \item The number of examples in $\mathcal{S}$ are less than the minimum number of examples 
defined for a split.
  
  \item The number of examples in $\mathcal{S}^l$ or $\mathcal{S}^r$ are less than the minimum 
number of examples defined for a leaf.
  
  \item The best splitting rule has a Gain lower than a defined threshold.
\end{itemize}


\subsubsection{Pruning techniques}
After a decision tree has been fully grown, there is the big chance that  the algorithm generates a 
large tree that is probably over fitting the  training data. In order to solve this in 
\citep{Breiman1984a} originally suggest the use of pruning techniques after the tree growing phase.
The overall objective of pruning is to eliminate branches that are not contributing
to the generalization accuracy of the \mbox{tree \citep{Rokach2010}.}
 
In general pruning techniques start from a fully grown tree, and recursively check if by 
eliminating 
a $Branch$ there is an $Improvement$ in the error rate $Err$ of the $Tree$.  There are two 
main 
methods to calculate the $Improvement$ of the error rate $Err$ of the $Tree$, cost-complexity 
and  error based pruning:
  
\begin{itemize}
\item Cost-complexity pruning:
  
Initially proposed by Breiman  \citep{Breiman1984a}, this method evaluate iterative if the removal 
of a $Branch$ improve the error rate $Err$ of a $Tree$, weighted by the difference of the 
number of leafs trees.
\begin{equation}\label{eq:8:pruning}
PC_{cc} = \frac{Err(EB(Tree,branch),S)-Err(Tree,\mathcal{S}) }
{\vert Tree\vert-\vert EB(Tree,branch)\vert}
\end{equation} 
where $Err(Tree,\mathcal{S})$ calculate the error rate of the $Tree$ in set $\mathcal{S}$ and 
$EB(Tree,branch)$ is an auxiliary function that removes $branch$ from $Tree$ and return a new 
$Tree$. At each iteration, the current $Tree$ is compared against all possible $Branches$.
    
\item Error based pruning:
  
This method proposed by Quinlan  \citep{Quinlan1992} is more pessimistic and instead of  using the 
error rate $Err$ calculate the upper bound of the confidence interval   of the error 
$Err_{UB}$ calculated assuming a normal distribution with a defined significance level 
$Z_\alpha$.
\begin{equation}
PC_{eb} = \frac{Err_{UB}(EB(Tree,branch),\mathcal{S})-Err_{UB}(Tree,\mathcal{S}) }
    {\vert Tree\vert-\vert EB(Tree,branch)\vert}
\end{equation} 
where 
\begin{align}
    Err_{UB}(Tree,&\mathcal{S})= Err(Tree,\mathcal{S}) & \nonumber \\
    &+Z_\alpha \sqrt{  
\frac{Err(Tree,\mathcal{S})(1-Err(Tree,\mathcal{S}))}{\vert \mathcal{S} \vert}} &
\end{align}   
Moreover, as a difference from cost-complexity pruning, in the error based method, at each 
iteration not all the branches are compared, but only the ones on the same level. Meaning leafs 
are only compared against other leafs that are in the same level of iterations when the $Tree$ 
was constructed. This allows for much faster pruning procedure.
\end{itemize}

In Algorithm~\ref{alg:8:tree_pruning}, the general method of pruning is shown. The pruning 
procedure consists in evaluating the $Improvement$ of eliminating all possible branches in a 
$Tree$, 
and then eliminate the one with the higher $Improvement$ and repeat the process until a stopping 
criteria is meet.

\begin{algorithm}\label{alg:8:tree_pruning}
Pseudocode of the tree pruning procedure.
\textnormal{
\begin{algorithmic}[1]
\Function{TreePruning}{$\mathcal{S}, Tree$}
    \State \# For all branches
    \For{$branch \  in \ Tree.branches$}
      \State \# Evaluate the pruning criteria
      \State $Improvements(branch) = $ \Call{PC}{$\mathcal{S}, Tree, branch$}
    \EndFor
    \State \# Select the branch to prune
    \State $branch^* = argmax_{branch} Improvements$
    \State \# Prune selected branch
    \State $Tree = $ \Call{EB}{$TreeTree,branch^*$}
    \State \# Recursively continue pruning the tree
    \State \Call{TreePruning}{$\mathcal{S}, Tree$}
    \State \Return $Tree$
\EndFunction
\end{algorithmic}
}
\end{algorithm}

 
\subsubsection{Categorical features}
When using continuous features the splitting is done by testing all possible thresholds on the 
database and  selecting the one that maximizes the desired measure. Nevertheless, this is not 
straight forward when using a  categorical feature, since testing all possible ways of binning the 
feature becomes prohibitively time  consuming the feature has many categories. Instead, a common 
method for doing this is to calculate $\pi_1$ of each category, then sort it, and applied the 
method as a continuous feature \citep{Marslan2009}.



\subsection{Decision tree algorithms}
% \begin{remark}{Decision tree algorithms}
Two main branches of decision trees has being studied in the last years. The main difference is 
the  impurity measure used when splitting. First the CART algorithm that is based in the Gini 
index, and  later the ID3 and C4.5 which uses the entropy measure.

\begin{itemize}
 \item 
  CART or classification and regression trees was introduced by Brieman et al. in 1984 
\citep{Breiman1984a}. It is based on using the Gini index as the impurity measure and the tree is 
grow until all examples in  each leaf belong to the same class. Afterwards, the tree is pruned 
using 
the cost-complexity  method \citep{Rokach2010,Marslan2009}.
  
\item ID3
  algorithm uses entropy as the impurity measure. The growing of the tree stop when all 
examples belong  of each leaf belongs to the same class. In ID3 no pruning is applied 
\citep{Quinlan1992}.
  
\item C4.5
  the extension of ID3 both proposed by Quinlan \citep{Quinlan1992}.  Both are similar 
regarding the measure used, but C4.5 define the stopping criteria during the growth process
  to be when the number of examples in a set is less than a threshold. Moreover, after the tree is 
created  a error based pruning is applied \citep{Rokach2010}.
\end{itemize} 
% \end{remark}


\section{Example-Dependent Cost-sensitive Decision Trees}
\label{sec:8:csdt}
     
  In this section, we first propose a new method to introduce the costs into the decision tree 
	induction stage, by creating new-cost based impurity measures. Afterwards, we propose a new 
	pruning method based on minimizing the cost as pruning criteria.

	\subsection{Cost-sensitive impurity measures}

		Standard impurity measures such as misclassification, entropy or Gini, take into account the 
		distribution of classes of each leaf to evaluate the predictive power of a splitting rule,
		leading to an impurity measure that is based on minimizing the misclassification rate. However, 
		as has been previously shown in Chapter~\ref{ch:6} and Chapter~\ref{ch:7}, minimizing 
  misclassification does not lead to the same results than minimizing cost. Instead, we are 
  interested in measuring how good is a splitting rule in terms of cost not only accuracy. For 
  doing that, we propose a new example-dependent cost based impurity measure that takes into 
  account the cost matrix of each example.

		We define a new cost-based impurity measure taking into account the costs when all the examples
		in a leaf are classified both as negative using $f_0$ and positive using $f_1$
		\begin{equation}\label{eq:8:cost_impurity}
			I_c(\mathcal{S}) = \min \bigg\{ Cost(f_0(\mathcal{S})), Cost(f_1(\mathcal{S})) \bigg\}.
		\end{equation}
		The objective of this measure is to evaluate the lowest expected cost of a splitting rule.
		Following the same logic, the classification of each set is calculated as the prediction that 
		leads to the lowest cost
	  \begin{equation}\label{eq:8:pred}
	    f(\mathcal{S}) = 
	    \begin{cases}
	      \phantom{-}0 \phantom{-} \mbox{if} \phantom{-} Cost(f_0(\mathcal{S})) \le 
        Cost(f_1(\mathcal{S}))\\
	      \phantom{-}1 \phantom{-}\mbox{otherwise}
	    \end{cases}
	  \end{equation}

		Finally, using the cost-based impurity, the splitting criteria cost based gain of using the 
		splitting rule $(\mathbf{x}^j,l^j)$ is calculated with (\ref{eq:8:gain}). 

	\subsection{Cost-sensitive pruning}
 
		Most of the literature in class-dependent cost-sensitive decision tree focuses on using the 
		misclassification costs during the construction of the algorithms \citep{Lomax2013}. Only few 
		algorithms such as AUCSplit \citep{Ferri2002} have \mbox{included} the costs both during and 
		after the construction of the tree. However, this approach only used the class-dependent costs, 
		and not the example-dependent costs.
 
		We propose a new example-dependent cost-based impurity measure, by replacing the error rate 
		$Err$ in (\ref{eq:8:pruning}) with the cost of using the $Tree$ on $\mathcal{S}$ i.e., by 
    replacing with 	$Cost(f(\mathcal{S}))$.
		\begin{equation}\label{eq:8:cost_pruning}
			PC_{c} = \frac{ Cost(f(\mathcal{S})) - Cost(f^*(\mathcal{S})) }
		  {\vert Tree\vert-\vert EB(Tree,node)\vert} ,
		\end{equation}
		where $f^*$ is the classifier of the tree without the selected node $EB(Tree,node)$.
 
		Using the new pruning criteria, nodes of the tree that do not contribute to the minimization of 
		the cost will be pruned, regardless of the impact of those nodes on the accuracy of the
		algorithm. This follows the same logic as in the proposed cost-based impurity measure, since 
		minimizing the misclassification is different than minimizing the cost, and in several 
		real-world applications the objectives align with the cost not with the misclassification error.
		
\section{Experiments}
\label{sec:8:experiments}

\begin{table}
  \centering
  \footnotesize
%   \tabcolsep 4pt
  \begin{tabular}{|c|l|c c|c c |c c|}
    \multicolumn{8}{c}{ }\\
    \cline{3-8}
    \multicolumn{2}{c}{}& \multicolumn{2}{|c}{Fraud Detection} & \multicolumn{2}{|c|}{Direct 
    Marketing} &
    \multicolumn{2}{|c|}{Credit Scoring}\\
    \hline
    set&Algorithm & $\%Sav$ & $F_1Score$ & $\%Sav$ & $F_1Score$ & $\%Sav$ & $F_1Score$\\
    \hline
    t&$DT_{not p}$& 31.76&0.4458&19.11&0.2976&18.95&0.3062\\
    &$DT_{err p}$&  31.76&0.4458&19.70&0.3147&18.95&0.3062\\
    &$DT_{cost p}$& 35.89&0.4590&28.08&0.3503&27.59&0.3743\\
    &$CSDT_{not p}$&  70.85&0.2529&69.00&0.2920&49.28&0.3669\\
    &$CSDT_{err p}$&  70.85&0.2529&68.97&0.3193&48.85&0.3669\\
    &$CSDT_{cost p}$& 71.16&0.2522&69.105&0.2878&49.39&0.3684\\
    \hline
    u&$DT_{not p}$& 52.39&0.1502&49.80&0.3374&48.91&0.2983\\
    &$DT_{err p}$&  52.39&0.1502&49.80&0.3374&48.91&0.2983\\
    &$DT_{cost p}$& 70.26&0.2333&53.20&0.3565&49.77&0.3286\\
    &$CSDT_{not p}$&  12.46&0.0761&64.21&0.2830&30.68&0.2061\\
    &$CSDT_{err p}$&  14.98&0.0741&66.21&0.2822&41.49&0.2564\\
    &$CSDT_{cost p}$& 15.01&0.0743&68.07&0.2649&44.89&0.2881\\
    \hline
    r&$DT_{not p}$& 34.39&0.4321&68.59&0.3135&48.97&0.3931\\
    &$DT_{err p}$&  34.39&0.4321&68.79&0.3196&48.97&0.3931\\
    &$DT_{cost p}$& 38.99&0.4478&69.27&0.3274&50.48&0.3501\\
    &$CSDT_{not p}$&  70.85&0.2529&66.87&0.2761&31.25&0.1940\\
    &$CSDT_{err p}$&  70.85&0.2529&67.47&0.2581&40.69&0.2529\\
    &$CSDT_{cost p}$& 71.09&0.2515&68.08&0.2642&44.51&0.2869\\
    \hline
    o&$DT_{not p}$& 31.72&0.4495&60.30&0.3674&47.39&0.3994\\
    &$DT_{err p}$&  31.72&0.4495&60.30&0.3674&47.39&0.3994\\
    &$DT_{cost p}$& 37.35&0.4575&69.44&0.3108&50.92&0.3977\\
    &$CSDT_{not p}$&  70.84&0.2529&68.75&0.2935&41.37&0.2205\\
    &$CSDT_{err p}$&  70.84&0.2529&68.75&0.2935&41.38&0.3457\\
    &$CSDT_{cost p}$& 71.09&0.2515&68.75&0.2935&41.65&0.2896\\
    \hline
  \end{tabular} 
    \caption{Results on the three datasets of the cost-sensitive and standard decision tree,    
  without pruning ($not p$), with error based pruning ($err p$), and with cost-sensitive pruning 
  technique ($cost p$). Estimated using the different training sets: training, under-sampling, 
  cost-proportionate rejection-sampling and cost-proportionate over-sampling}
  \label{tab:8:1}
\end{table}

In this section we present the experimental results. First, we evaluate the performance of the 
proposed $CSDT$ algorithm and compare it against a classical decision tree ($DT$). We evaluate 
the different trees using them without pruning ($not p$), with error based pruning ($err p$), 
and also with the proposed cost-sensitive pruning technique ($cost p$). The different algorithms 
are trained using the training ($t$), under-sampling ($u$), cost-proportionate rejection-sampling 
($r$), and cost-proportionate over-sampling ($o$) datasets. We use the decision trees 
implementation of \textit{Scikit-learn} \citep{Pedregosa2011}, and the \textit{CostCla} library, 
see Appendix~\ref{ch:A}, for the cost-sensitive algorithms.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.8]{ch8_fig2}
  \caption{Results of the $DT$ and the $CSDT$. For both algorithms, the results are 
  calculated with and without both types of pruning  criteria.}
\label{fig:8:2}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[scale=0.8]{ch8_fig3}
  \caption{Average savings on the three datasets of the different cost-sensitive and standard 
  decision tree,  estimated using the different training sets: training, under-sampling, 
  cost-proportionate rejection-sampling  and cost-proportionate over-sampling.}
  \label{fig:8:3}
\end{figure}

For the experiments we use three datasets from three different real world example-dependent 
cost-sensitive problems: Credit card fraud detection (see Section~\ref{sec:4:fraud}), credit 
scoring (see Section~\ref{sec:4:creditscoring}) and direct marketing (see 
Section~\ref{sec:5:directmarketing}). The different datasets are summarized 
in \tablename{~\ref{tab:4:databases}} and \tablename{~\ref{tab:5:databases}}.

We evaluate a decision tree constructed using the Gini impurity measure, with and without the 
pruning defined in (\ref{eq:8:pruning}). We also apply    the cost-based pruning procedure given 
in (\ref{eq:8:cost_pruning}). Lastly, we compared against     the proposed $CSDT$ constructed 
using the cost-based impurity measure defined in (\ref{eq:8:cost_impurity}), using the two 
pruning procedures.

In \figurename{~\ref{fig:8:2}}, the results using the three databases are shown. In particular we 
first evaluate the impact of the algorithms when trained using the training set. There is a clear 
difference between the savings of the $DT$ and the $CSDT$ algorithms. However, that difference 
is not observable on the $F_1Score$ results. Since the $CSDT$ is focused on maximizing the 
savings not the accuracy or $F_1Score$. There is a small increase in savings when using the 
$DT$ with cost-sensitive pruning. Nevertheless, in the case of the $CSDT$ algorithm, there is 
no change when using any pruning procedure, neither in savings or $F_1Score$.

In addition, we also evaluate the algorithms on the different sets, under-sampling, 
rejection-sampling and over-sampling. The results are shown in \tablename{~\ref{tab:8:1}}. 
Moreover, in \figurename{~\ref{fig:8:3}}, the average results of the different algorithms measured 
by savings is shown. The best results are found when using the training set. When using the 
under-sampling set there is a decrease in savings of the $CSDT$ algorithm. Lastly, in the case of 
the cost-proportionate sampling sets, there is a small increase in savings when using the $CSDT$ 
algorithm. 
 
\begin{figure}
  \centering
  \includegraphics[scale=0.8]{ch8_fig4}
  \caption{Average tree size (a) and training time (b), of the different cost-sensitive and 
  standard decision tree, estimated using the different training sets.}
  \label{fig:8:4}
\end{figure}

\begin{table}
  \centering
  \footnotesize
  \begin{tabular}{|c|l|c c|c c|c c|}
    \cline{3-8}
    \multicolumn{2}{c}{}& \multicolumn{2}{|c|}{Fraud Detection} & \multicolumn{2}{|c|}{Direct 
    Marketing} 
    & \multicolumn{2}{|c|}{Credit Scoring}\\
    \hline
    set&Algorithm & $\vert Tree\vert$ & Time& $\vert Tree\vert$ & Time& $\vert Tree\vert$ & Time\\
    \hline
    t &$DT_{not p}$&488&2.45&298&1.58&292&1.58\\
    &$DT_{err p}$&488&3.90&298&2.13&292&2.13\\
    &$DT_{cost p}$&446&19.19&291&5.23&280&5.23\\
    &$CSDT_{not p}$&89&1.47&51&0.40&69&0.40\\
    &$CSDT_{err p}$&88&1.87&51&0.64&69&0.64\\
    &$CSDT_{cost p}$&89&1.74&51&0.45&69&0.45\\
    \hline
    u &$DT_{not p}$&308&1.10&198&1.00&167&1.00\\
    &$DT_{err p}$&308&1.43&198&1.14&167&1.14\\
    &$DT_{cost p}$&153&2.59&190&1.34&142&1.34\\
    &$CSDT_{not p}$&14&0.20&23&0.17&42&0.17\\
    &$CSDT_{err p}$&14&0.23&23&0.19&42&0.19\\
    &$CSDT_{cost p}$&14&0.24&23&0.18&42&0.18\\
    \hline
    r &$DT_{not p}$&268&0.98&181&0.90&267&0.90\\
    &$DT_{err p}$&268&1.24&181&0.95&267&0.95\\
    &$DT_{cost p}$&153&2.48&162&1.20&261&1.20\\
    &$CSDT_{not p}$&18&0.22&10&0.07&70&0.07\\
    &$CSDT_{err p}$&18&0.23&10&0.07&70&0.07\\
    &$CSDT_{cost p}$&18&0.23&10&0.07&70&0.07\\
    \hline
    o &$DT_{not p}$&425&2.30&340&1.80&277&1.80\\
    &$DT_{err p}$&425&3.98&340&2.65&277&2.65\\
    &$DT_{cost p}$&364&10.15&288&5.99&273&5.99\\
    &$CSDT_{not p}$&37&1.58&51&0.38&70&0.38\\
    &$CSDT_{err p}$&37&1.90&51&0.45&70&0.45\\
    &$CSDT_{cost p}$&37&1.98&51&0.42&70&0.42\\
    \hline
  \end{tabular} 
  \caption{Training time and tree size of the different cost-sensitive and standard decision 
  tree, estimated using the different training sets: training, under-sampling, 
  cost-proportionate rejection-sampling and cost-proportionate over-sampling, for the three 
  databases.}
  \label{tab:8:2}
\end{table}

  
Finally, we also analyze the different models taking into account the complexity and the 
training time. In particular we evaluate the size of each $Tree$. In \tablename{~\ref{tab:8:2}}, 
and \figurename{~\ref{fig:8:4}}, the results are shown. The $CSDT$ algorithm creates significantly 
smaller trees, which leads to a lower training time. In particular this is a result of using the 
non weighted gain, the $CSDT$ only accepts splitting rules that contribute to the overall reduction 
of the cost, which is not the case if instead the weighted gain was used. Even that the $DT$ 
with cost pruning, produce a good result measured by savings, it is the one that takes the 
longer to estimate. Since the algorithm first creates a big decision tree using the $Gini$ 
impurity, and then attempt to find a smaller tree taking into account the cost. Measured by 
training time, the $CSDT$ is by all means faster to train than the $DT$ algorithm, leading to 
an algorithm that not only gives better results measured by savings but also one that can be 
trained much quicker than the standard $DT$.
