\chapter{\textit{Costcla}: A Cost-sensitive classification library in \textbf{Python}}\label{ch:9}


\begin{remark}{Abstract}
\textit{CostCla} is a \textbf{Python} open source cost-sensitive classification library built on 
top of \textit{Scikit-learn}, \textit{Pandas} and \textit{Numpy}. \textit{CostCla} provides a wide 
range of state-of-the-art example-dependent cost-sensitive methods for classification tasks. Source 
code, binaries and documentation are distributed under 3-Clause BSD license in the website 
\url{http://albahnsen.com/CostSensitiveClassification/}.
\end{remark}
  
\section{Introduction}
\label{sec:9:intro}

Classification, in the context of machine learning, deals with the problem of predicting the class 
of a set of examples given their features. Traditionally, classification methods aim at minimizing 
the misclassification of examples, in which an example is misclassified if the predicted class is 
different from the true class. 
\textbf{Python} offers several machine learning packages, some including classification algorithms 
like \textit{Scikit-learn} \citep{Pedregosa2011}, \textit{PyBrain} \citep{Schaul2010}, 
\textit{PyMC} \citep{Patil2010}, \textit{mlpy} \citep{Albanese2012} and \textit{Orange} 
\citep{Demsar2013}.

Nevertheless, all this packages are based on a cost-insensitive framework, in which all 
misclassification errors carry the same cost. This is not the case in many real-world applications. 
For example in credit card fraud detection, failing to detect a fraudulent transaction may have 
an economical impact from a few to thousands of Euros, depending on the particular transaction 
and card holder \citep{Ngai2011a}. In churn modeling, a model is used for predicting which
customers are more likely to abandon a service provider. In this context, failing to identify a 
profitable or unprofitable churner has a significant different economic 
result~\citep{Verbraken2013}. Similarly, in direct marketing, wrongly predicting that a customer 
will not accept an offer when in fact he will, may have different financial impact, as not all 
customers generate the same profit \citep{Zadrozny2003}. Lastly, in credit scoring, accepting 
loans from bad customers does not have the same economical loss, since customers have different 
credit lines, therefore, different profit \citep{Verbraken2014}.


In this Chapter, we present \textit{CostCla} a cost-sensitive classification library in 
\textbf{Python}. The library incorporates several cost-sensitive algorithms. Moreover, we show show 
the huge differences in profit when using traditional machine learning algorithms versus 
cost-sensitive algorithms, on several real-world databases.

\section{Library overview}

The core of the library consists of a number of state-of-the-art example-dependent cost-sensitive 
classification algorithms, such as cost-proportionate rejection-sampling \citep{Zadrozny2003}, 
cost-proportionate over-sampling \citep{Elkan2001}, Bayes minimum risk 
\citep{CorreaBahnsen2013,CorreaBahnsen2014}, cost-sensitive logistic regression 
\citep{CorreaBahnsen2014b}, cost-sensitive decision trees \citep{CorreaBahnsen2015} and the 
cost-sensitive ensemble methods: cost-sensitive bagging, cost-sensitive pasting, cost-sensitive 
random forest and cost-sensitive random patches \citep{CorreaBahnsen2015b}.

Moreover, the library also includes three different example-dependent cost-sensitive databases. In 
particular two credit scoring databases, and one direct marketing database.

\section{Usage}

In this Section we provided a quick example of the usage of the \textit{CostCla}  library, and 
compare the results of different algorithms using a credit scoring database.

\vskip0.3cm
\noindent \textbullet\ Prepare dataset and load libraries

\vskip0.3cm
\begin{pythoncode}
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import train_test_split
from costcla.datasets import load_creditscoring2
from costcla.sampling import cost_sampling
from costcla.metrics import savings_score
from costcla import models
data = load_creditscoring2()
X_train, X_test, y_train, y_test, 
cost_mat_train, cost_mat_test = \
train_test_split(data.data, data.target, data.cost_mat)
\end{pythoncode}

\vskip0.3cm
\noindent\textbullet\ Random forest

\vskip0.3cm
\begin{pythoncode}
f_RF = RandomForestClassifier()
f_RF.fit(X_train, y_train)
y_pred = f_RF.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.042197359989}

\newpage
\noindent \textbullet\ Cost-proportionate rejection sampling

\vskip0.3cm
\begin{pythoncode}
X_cps_r, y_cps_r, cost_mat_cps_r =  \ 
cost_sampling(X_train, y_train, cost_mat_train, 
              method='RejectionSampling')
f_RF.fit(X_train, y_train)
y_pred = f_RF.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.280743761779}

\vskip0.3cm
\noindent \textbullet\ Bayes minimum risk

\vskip0.3cm
\begin{pythoncode}
f_RF.fit(X_train, y_train)
y_prob_test = f_RF.predict_proba(X_test)
f_BMR = models.BayesMinimumRiskClassifier()
f_BMR.fit(y_test, y_prob_test)
y_pred = f_BMR.predict(y_prob_test, cost_mat_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.285102564249}

\vskip0.3cm
\noindent \textbullet\ Cost-sensitive decision tree

\vskip0.3cm
\begin{pythoncode}
f_CSDT = models.CSDecisionTreeClassifier()
f_CSDT.fit(X_train, y_train, cost_mat_train)
y_pred = f_CSDT.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.289489571352}

\vskip0.3cm
\noindent \textbullet\ Cost-sensitive random patches

\vskip0.3cm
\begin{pythoncode}
f_CSRP = models.CSRandomPatchesClassifier()
f_CSRP.fit(X_train, y_train, cost_mat_train)
y_pred = f_CSRP.predict(X_test)
print savings_score(y_test, y_pred, cost_mat_test)
\end{pythoncode}
\textbf{0.306607400467}

\newpage
In the \tablename{~\ref{tab:9:1}} we summarized the results of the different 
algorithms. It is observed that the library follows the same API structure of 
\textit{Scikit-learn}, therefore, making \textit{CostCla} highly compatible with the most widely 
used machine learning library in 
\textbf{Python}\footnote{\url{
http://machinelearningmastery.com/best-programming-language-for-machine-learning/}}. Moreover, the 
results highlight the importance of using example-dependent cost-sensitive methods when 

\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{l|c}
      Algorithm & Savings \\
      \hline
      Random forest & 0.0422 \\
      Cost-proportionate rejection sampling & 0.2807\\
      Bayes minimum risk & 0.2851\\
      Cost-sensitive decision tree & 0.2895\\
      Cost-sensitive random patches & 0.3066\\   
    \end{tabular}
    \caption{Results of the different algorithms using the \textit{CostCla} library}
    \label{tab:9:1}
  \end{table}  

\section{Installation}

\textit{CostCla} requires some prerequisite packages to be previously installed. Nevertheless, all 
are cross-platform an freely available online:
\begin{itemize}
 \item \textbf{Python} version $\ge$ 2.7
 \item \textit{Numpy} version $\ge$ 1.8.0
 \item \textit{Pandas} version $\ge$ 0.14.0
 \item \textit{Scikit-learn} version $\ge$ 0.15.0b2
 \item \textit{pyea} version $\ge$ 0.1
\end{itemize}

\noindent Then, the easiest way to install \textit{CostCla}  is with \textit{pip}:
\vskip0.3cm
\begin{pythoncode}
pip install costcla
\end{pythoncode}

\section{Conclusion}
\textit{CostCla} is a easy to use \textbf{Python} library for example-dependent cost-sensitive 
classification problems. It includes many example-dependent cost-sensitive algorithms. Since 
it is part of the scientific \textbf{Python} ecosystem, it can be easily integrated with other 
machine learning libraries. Future work includes adding more cost-sensitive databases and 
algorithms, and support for \textbf{Python} $\ge$ 3.4. 
